{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling with recurrent neural networks using Keras\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will to see how to build a recurrent neural network (RNN) language model that learns the relation between words in text, using the Keras library for machine learning. We will then see how this model can be used to compute the probability of a sequence, as well as generate new sequences.\n",
    "\n",
    "### Language Modeling\n",
    "\n",
    "A language model is a model of the probability of word sequences. These models are useful for a variety of tasks, such as ones that require selecting the most likely output from a set of candidates provided by a speech recognition or machine translation system, for example. Here, we will to see how a language model can be used to generate sequences, in particular the endings of stories. Language generation is a difficult research problem which is generally addressed by more complex models than the one shown here.\n",
    "\n",
    "Traditionally, the most well-known approach to language modeling relies on n-grams. The limitation of n-gram language models is that they only explicitly model the probability of a sequence of *n* words. In contrast, RNNs can model longer sequences and thus typically are better at predicting which words will appear in a sequence. See the [chapter in Jurafsky & Martin's *Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/4.pdf) to learn more about traditional approaches to language modeling. \n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural language processing tasks. At a high level, RNN encode sequences via a set of parameters (weights) that are optimized to predict some output variable. The focus of this notebook is on the code needed to assemble a model in Keras, as well as some data processing tools that facilitate building the model. \n",
    "\n",
    "If you understand how to structure the input and output of the model, and know the fundamental concepts in machine learning, then a high-level understanding of how an RNN works is sufficient for using Keras. You'll see that most of the code here is actually just data manipulation, and I'll visualize each step in this process. The code used to assemble the RNN itself is more minimal. It is of course useful to know the technical details of the RNN, so you can theorize on the results and innovate the model to make it better. For a better understanding of RNNs and neural networks in general, see the resources at the bottom of the notebook.\n",
    "\n",
    "Here an RNN will be used as a language model, which can predict which word is likely to occur next in a text given the words before it.\n",
    "\n",
    "### Installation of spaCy\n",
    "\n",
    "There are two tools widely used in the NLP world. These are two libraries called respectively spacy and nltk. The choice of spacy is based on its ease of implementation.\n",
    "> If you want to know the differences between spacy and nltk, consult this article. \n",
    "> https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2\n",
    "\n",
    "\n",
    "***Installation via pip***\n",
    "\n",
    "````\n",
    "pip3 install -U spacy\n",
    "````\n",
    "\n",
    "***Installation via conda***\n",
    "````\n",
    "conda install -c conda-forge spacy\n",
    "````\n",
    "\n",
    "**Then you must install the language module !**\n",
    "\n",
    "````\n",
    "python3 -m spacy download en_core_web_sm\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Our research is on story generation, so we've selected a dataset of stories as the text to be modeled by the RNN. They come from the [ROCStories](http://cs.rochester.edu/nlp/rocstories/)  dataset, which consists of thousands of five-sentence stories about everyday life events. Here the model will be trained to predict each word in the story based on the preceding words. Then we will use the trained model to generate the final sentence in a set of stories not observed during training. The full dataset is available at the above link and just requires filling out a form to get access. Here, we will use a sample of 100 stories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to import this module because Keras was generating errors for me, \n",
    "# it is possible that it works for you without this module.\n",
    "import os\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "from __future__ import print_function #Python 2/3 compatibility for print statements\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 300) #widen pandas rows display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we'll load the data that's in csv with **pandas**. You should know how to do it!  \n",
    "\n",
    "**Exercise :** Load the dataset with the path ``dataset/example_train_stories.csv`` and save it in a variable ``train_stories``. Load only the first 100 stories to relieve your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dan's parents were overweight. Dan was overwei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrie had just learned how to ride a bike. Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morgan enjoyed long walks on the beach. She an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane was working at a diner. Suddenly, a custo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was talking to my crush today. She continued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank had been drinking beer. He got a call fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave was in the Bahamas on vacation. He decide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny enjoyed going to the beach. As she stepp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sally was happy when her widowed mom found a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dan hit his golf ball and watched it go. The b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story\n",
       "0  Dan's parents were overweight. Dan was overwei...\n",
       "1  Carrie had just learned how to ride a bike. Sh...\n",
       "2  Morgan enjoyed long walks on the beach. She an...\n",
       "3  Jane was working at a diner. Suddenly, a custo...\n",
       "4  I was talking to my crush today. She continued...\n",
       "5  Frank had been drinking beer. He got a call fr...\n",
       "6  Dave was in the Bahamas on vacation. He decide...\n",
       "7  Sunny enjoyed going to the beach. As she stepp...\n",
       "8  Sally was happy when her widowed mom found a n...\n",
       "9  Dan hit his golf ball and watched it go. The b..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the training dataset'''\n",
    "### ENTER YOUR CODE HERE ( 1 line)\n",
    "import pandas as pd\n",
    "train_stories = pd.read_csv('dataset/example_train_stories.csv')\n",
    "### END\n",
    "train_stories[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "The model we'll create is a word-based language model, which means each input unit is a single word (alternatively, some language models learn subword units like characters).\n",
    "\n",
    "###  Tokenization\n",
    "\n",
    "The first pre-processing step is to tokenize each of the stories into (lowercased) individual words, since the RNN will encode the stories word by word. For this we will use [spaCy](https://spacy.io/), which is a fast and extremely user-friendly library that performs various language processing tasks. Once you load a spaCy model for a particular language, you can provide any text as input to the model (e.g. encoder(text)) and access its linguistic features.\n",
    "\n",
    "**Exercise:**: Load the English ('en') language with spacy. Check the documentation if necessary. Save it in the encoder variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split texts into lists of words (tokens)'''\n",
    "\n",
    "import spacy\n",
    "\n",
    "###ENTER YOUR CODE HERE 1 line\n",
    "### Load the English ('en') language with spacy\n",
    "\n",
    "\n",
    "encoder = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a text_to_to_token function because we will reuse this piece of code for the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(text_seqs):\n",
    "    token_seqs = [[word.lower_ for word in encoder(text_seq)] for text_seq in text_seqs]\n",
    "    return token_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can read, for each story we will break down the sentences and create a vector containing each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello',\n",
       "  ',',\n",
       "  'my',\n",
       "  'name',\n",
       "  'is',\n",
       "  'ludo',\n",
       "  '.',\n",
       "  'i',\n",
       "  'learn',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  '.',\n",
       "  'and',\n",
       "  'i',\n",
       "  'like',\n",
       "  'it',\n",
       "  '!'],\n",
       " ['this',\n",
       "  'is',\n",
       "  'the',\n",
       "  'second',\n",
       "  'story',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'beginning',\n",
       "  ',',\n",
       "  'no',\n",
       "  'end',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.DataFrame(['Hello, my name is Ludo. I learn deep learning . And i like it !', \n",
    "                     'This is the second story. There is no beginning, no end.'])\n",
    "example = text_to_tokens(example[0])\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** \"Tokenize\" the stories in variable ``train_stories['Story']``. Save the tokens by creating a new column in train_storiescalled ``Tokenized_Story``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ( 1 line)Âµ\n",
    "train_stories['Tokenized_Story'] = text_to_tokens(train_stories['Story'])\n",
    "\n",
    "\n",
    "\n",
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure you have that by posting the first 10 stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>Tokenized_Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dan's parents were overweight. Dan was overwei...</td>\n",
       "      <td>[dan, 's, parents, were, overweight, ., dan, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrie had just learned how to ride a bike. Sh...</td>\n",
       "      <td>[carrie, had, just, learned, how, to, ride, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morgan enjoyed long walks on the beach. She an...</td>\n",
       "      <td>[morgan, enjoyed, long, walks, on, the, beach,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane was working at a diner. Suddenly, a custo...</td>\n",
       "      <td>[jane, was, working, at, a, diner, ., suddenly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was talking to my crush today. She continued...</td>\n",
       "      <td>[i, was, talking, to, my, crush, today, ., she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank had been drinking beer. He got a call fr...</td>\n",
       "      <td>[frank, had, been, drinking, beer, ., he, got,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dave was in the Bahamas on vacation. He decide...</td>\n",
       "      <td>[dave, was, in, the, bahamas, on, vacation, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny enjoyed going to the beach. As she stepp...</td>\n",
       "      <td>[sunny, enjoyed, going, to, the, beach, ., as,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sally was happy when her widowed mom found a n...</td>\n",
       "      <td>[sally, was, happy, when, her, widowed, mom, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dan hit his golf ball and watched it go. The b...</td>\n",
       "      <td>[dan, hit, his, golf, ball, and, watched, it, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  \\\n",
       "0  Dan's parents were overweight. Dan was overwei...   \n",
       "1  Carrie had just learned how to ride a bike. Sh...   \n",
       "2  Morgan enjoyed long walks on the beach. She an...   \n",
       "3  Jane was working at a diner. Suddenly, a custo...   \n",
       "4  I was talking to my crush today. She continued...   \n",
       "5  Frank had been drinking beer. He got a call fr...   \n",
       "6  Dave was in the Bahamas on vacation. He decide...   \n",
       "7  Sunny enjoyed going to the beach. As she stepp...   \n",
       "8  Sally was happy when her widowed mom found a n...   \n",
       "9  Dan hit his golf ball and watched it go. The b...   \n",
       "\n",
       "                                     Tokenized_Story  \n",
       "0  [dan, 's, parents, were, overweight, ., dan, w...  \n",
       "1  [carrie, had, just, learned, how, to, ride, a,...  \n",
       "2  [morgan, enjoyed, long, walks, on, the, beach,...  \n",
       "3  [jane, was, working, at, a, diner, ., suddenly...  \n",
       "4  [i, was, talking, to, my, crush, today, ., she...  \n",
       "5  [frank, had, been, drinking, beer, ., he, got,...  \n",
       "6  [dave, was, in, the, bahamas, on, vacation, .,...  \n",
       "7  [sunny, enjoyed, going, to, the, beach, ., as,...  \n",
       "8  [sally, was, happy, when, her, widowed, mom, f...  \n",
       "9  [dan, hit, his, golf, ball, and, watched, it, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories[['Story','Tokenized_Story']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this: \n",
    "\n",
    "![dataframe](../img/capt01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lexicon\n",
    "\n",
    "Then we need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Each tokenized word in the stories is added to the lexicon, and then each word is mapped to a numerical index that can be read by the model. Since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times, and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. In the example code, the min_freq parameter is set to 1, so the lexicon will contain all unique words in the training set. When assigning the indices, the number 1 will represent unknown words. The number 0 will represent \"empty\" word slots, which is explained below. Therefore \"real\" words will have indices of 2 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a token_count(sequence_text) function that will count the number of times the word appears in the text sequence. This function should return a dictionary that should look like this: `` {'hello': 1,','': 2,''my': 1,''name'': 1}``. So the word in index, and the number of times the number in value appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dan',\n",
       " \"'s\",\n",
       " 'parents',\n",
       " 'were',\n",
       " 'overweight',\n",
       " '.',\n",
       " 'dan',\n",
       " 'was',\n",
       " 'overweight',\n",
       " 'as',\n",
       " 'well',\n",
       " '.',\n",
       " 'the',\n",
       " 'doctors',\n",
       " 'told',\n",
       " 'his',\n",
       " 'parents',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unhealthy',\n",
       " '.',\n",
       " 'his',\n",
       " 'parents',\n",
       " 'understood',\n",
       " 'and',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'change',\n",
       " '.',\n",
       " 'they',\n",
       " 'got',\n",
       " 'themselves',\n",
       " 'and',\n",
       " 'dan',\n",
       " 'on',\n",
       " 'a',\n",
       " 'diet',\n",
       " '.',\n",
       " 'carrie',\n",
       " 'had',\n",
       " 'just',\n",
       " 'learned',\n",
       " 'how',\n",
       " 'to',\n",
       " 'ride',\n",
       " 'a',\n",
       " 'bike',\n",
       " '.',\n",
       " 'she',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'bike',\n",
       " 'of',\n",
       " 'her',\n",
       " 'own',\n",
       " '.',\n",
       " 'carrie',\n",
       " 'would',\n",
       " 'sneak',\n",
       " 'rides',\n",
       " 'on',\n",
       " 'her',\n",
       " 'sister',\n",
       " \"'s\",\n",
       " 'bike',\n",
       " '.',\n",
       " 'she',\n",
       " 'got',\n",
       " 'nervous',\n",
       " 'on',\n",
       " 'a',\n",
       " 'hill',\n",
       " 'and',\n",
       " 'crashed',\n",
       " 'into',\n",
       " 'a',\n",
       " 'wall',\n",
       " '.',\n",
       " 'the',\n",
       " 'bike',\n",
       " 'frame',\n",
       " 'bent',\n",
       " 'and',\n",
       " 'carrie',\n",
       " 'got',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'gash',\n",
       " 'on',\n",
       " 'her',\n",
       " 'leg',\n",
       " '.',\n",
       " 'morgan',\n",
       " 'enjoyed',\n",
       " 'long',\n",
       " 'walks',\n",
       " 'on',\n",
       " 'the',\n",
       " 'beach',\n",
       " '.',\n",
       " 'she',\n",
       " 'and',\n",
       " 'her',\n",
       " 'boyfriend',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'go',\n",
       " 'for',\n",
       " 'a',\n",
       " 'long',\n",
       " 'walk',\n",
       " '.',\n",
       " 'after',\n",
       " 'walking',\n",
       " 'for',\n",
       " 'over',\n",
       " 'a',\n",
       " 'mile',\n",
       " ',',\n",
       " 'something',\n",
       " 'happened',\n",
       " '.',\n",
       " 'morgan',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'propose',\n",
       " 'to',\n",
       " 'her',\n",
       " 'boyfriend',\n",
       " '.',\n",
       " 'her',\n",
       " 'boyfriend',\n",
       " 'was',\n",
       " 'upset',\n",
       " 'he',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'propose',\n",
       " 'to',\n",
       " 'her',\n",
       " 'first',\n",
       " '.',\n",
       " 'jane',\n",
       " 'was',\n",
       " 'working',\n",
       " 'at',\n",
       " 'a',\n",
       " 'diner',\n",
       " '.',\n",
       " 'suddenly',\n",
       " ',',\n",
       " 'a',\n",
       " 'customer',\n",
       " 'barged',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'counter',\n",
       " '.',\n",
       " 'he',\n",
       " 'began',\n",
       " 'yelling',\n",
       " 'about',\n",
       " 'how',\n",
       " 'long',\n",
       " 'his',\n",
       " 'food',\n",
       " 'was',\n",
       " 'taking',\n",
       " '.',\n",
       " 'jane',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'react',\n",
       " '.',\n",
       " 'luckily',\n",
       " ',',\n",
       " 'her',\n",
       " 'coworker',\n",
       " 'intervened',\n",
       " 'and',\n",
       " 'calmed',\n",
       " 'the',\n",
       " 'man',\n",
       " 'down',\n",
       " '.',\n",
       " 'i',\n",
       " 'was',\n",
       " 'talking',\n",
       " 'to',\n",
       " 'my',\n",
       " 'crush',\n",
       " 'today',\n",
       " '.',\n",
       " 'she',\n",
       " 'continued',\n",
       " 'to',\n",
       " 'complain',\n",
       " 'about',\n",
       " 'guys',\n",
       " 'flirting',\n",
       " 'with',\n",
       " 'her',\n",
       " '.',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'agree',\n",
       " 'with',\n",
       " 'what',\n",
       " 'she',\n",
       " 'says',\n",
       " 'and',\n",
       " 'listened',\n",
       " 'to',\n",
       " 'her',\n",
       " 'patiently',\n",
       " '.',\n",
       " 'after',\n",
       " 'i',\n",
       " 'got',\n",
       " 'home',\n",
       " ',',\n",
       " 'i',\n",
       " 'got',\n",
       " 'a',\n",
       " 'text',\n",
       " 'from',\n",
       " 'her',\n",
       " '.',\n",
       " 'she',\n",
       " 'asked',\n",
       " 'if',\n",
       " 'we',\n",
       " 'can',\n",
       " 'hang',\n",
       " 'out',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'frank',\n",
       " 'had',\n",
       " 'been',\n",
       " 'drinking',\n",
       " 'beer',\n",
       " '.',\n",
       " 'he',\n",
       " 'got',\n",
       " 'a',\n",
       " 'call',\n",
       " 'from',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " ',',\n",
       " 'asking',\n",
       " 'where',\n",
       " 'he',\n",
       " 'was',\n",
       " '.',\n",
       " 'frank',\n",
       " 'suddenly',\n",
       " 'realized',\n",
       " 'he',\n",
       " 'had',\n",
       " 'a',\n",
       " 'date',\n",
       " 'that',\n",
       " 'night',\n",
       " '.',\n",
       " 'since',\n",
       " 'frank',\n",
       " 'was',\n",
       " 'already',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'drunk',\n",
       " ',',\n",
       " 'he',\n",
       " 'could',\n",
       " 'not',\n",
       " 'drive',\n",
       " '.',\n",
       " 'frank',\n",
       " 'spent',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'night',\n",
       " 'drinking',\n",
       " 'more',\n",
       " 'beers',\n",
       " '.',\n",
       " 'dave',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bahamas',\n",
       " 'on',\n",
       " 'vacation',\n",
       " '.',\n",
       " 'he',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'go',\n",
       " 'snorkeling',\n",
       " 'on',\n",
       " 'his',\n",
       " 'second',\n",
       " 'day',\n",
       " '.',\n",
       " 'while',\n",
       " 'snorkeling',\n",
       " ',',\n",
       " 'he',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'cave',\n",
       " 'up',\n",
       " 'ahead',\n",
       " '.',\n",
       " 'he',\n",
       " 'went',\n",
       " 'into',\n",
       " 'the',\n",
       " 'cave',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'was',\n",
       " 'terrified',\n",
       " 'when',\n",
       " 'he',\n",
       " 'found',\n",
       " 'a',\n",
       " 'shark',\n",
       " '!',\n",
       " 'dave',\n",
       " 'swam',\n",
       " 'away',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'he',\n",
       " 'could',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'shark',\n",
       " 'caught',\n",
       " 'and',\n",
       " 'ate',\n",
       " 'dave',\n",
       " '.',\n",
       " 'sunny',\n",
       " 'enjoyed',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'beach',\n",
       " '.',\n",
       " 'as',\n",
       " 'she',\n",
       " 'stepped',\n",
       " 'out',\n",
       " 'of',\n",
       " 'her',\n",
       " 'car',\n",
       " ',',\n",
       " 'she',\n",
       " 'realized',\n",
       " 'she',\n",
       " 'forgot',\n",
       " 'something',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'quite',\n",
       " 'sunny',\n",
       " 'and',\n",
       " 'she',\n",
       " 'forgot',\n",
       " 'her',\n",
       " 'sunglasses',\n",
       " '.',\n",
       " 'sunny',\n",
       " 'got',\n",
       " 'back',\n",
       " 'into',\n",
       " 'her',\n",
       " 'car',\n",
       " 'and',\n",
       " 'heading',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'mall',\n",
       " '.',\n",
       " 'sunny',\n",
       " 'found',\n",
       " 'some',\n",
       " 'sunglasses',\n",
       " 'and',\n",
       " 'headed',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'beach',\n",
       " '.',\n",
       " 'sally',\n",
       " 'was',\n",
       " 'happy',\n",
       " 'when',\n",
       " 'her',\n",
       " 'widowed',\n",
       " 'mom',\n",
       " 'found',\n",
       " 'a',\n",
       " 'new',\n",
       " 'man',\n",
       " '.',\n",
       " 'she',\n",
       " 'discovered',\n",
       " 'her',\n",
       " 'siblings',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " 'sally',\n",
       " 'flew',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'her',\n",
       " 'mom',\n",
       " 'and',\n",
       " 'her',\n",
       " 'mom',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'husband',\n",
       " '.',\n",
       " 'although',\n",
       " 'her',\n",
       " 'mom',\n",
       " 'was',\n",
       " 'obviously',\n",
       " 'in',\n",
       " 'love',\n",
       " ',',\n",
       " 'he',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'like',\n",
       " 'her',\n",
       " 'dad',\n",
       " '.',\n",
       " 'sally',\n",
       " 'went',\n",
       " 'home',\n",
       " 'and',\n",
       " 'wondered',\n",
       " 'about',\n",
       " 'her',\n",
       " 'parents',\n",
       " \"'\",\n",
       " 'marriage',\n",
       " '.',\n",
       " 'dan',\n",
       " 'hit',\n",
       " 'his',\n",
       " 'golf',\n",
       " 'ball',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'it',\n",
       " 'go',\n",
       " '.',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'bounced',\n",
       " 'on',\n",
       " 'the',\n",
       " 'grass',\n",
       " 'and',\n",
       " 'into',\n",
       " 'the',\n",
       " 'sand',\n",
       " 'trap',\n",
       " '.',\n",
       " 'dan',\n",
       " 'pretended',\n",
       " 'that',\n",
       " 'his',\n",
       " 'ball',\n",
       " 'actually',\n",
       " 'landed',\n",
       " 'on',\n",
       " 'the',\n",
       " 'green',\n",
       " '.',\n",
       " 'his',\n",
       " 'friends',\n",
       " 'were',\n",
       " 'not',\n",
       " 'paying',\n",
       " 'attention',\n",
       " 'so',\n",
       " 'they',\n",
       " 'believed',\n",
       " 'him',\n",
       " '.',\n",
       " 'dan',\n",
       " 'snuck',\n",
       " 'a',\n",
       " 'ball',\n",
       " 'on',\n",
       " 'the',\n",
       " 'green',\n",
       " 'and',\n",
       " 'made',\n",
       " 'his',\n",
       " 'putt',\n",
       " 'from',\n",
       " '10',\n",
       " 'feet',\n",
       " '.',\n",
       " 'josh',\n",
       " 'had',\n",
       " 'a',\n",
       " 'parrot',\n",
       " 'that',\n",
       " 'talked',\n",
       " '.',\n",
       " 'he',\n",
       " 'brought',\n",
       " 'his',\n",
       " 'parrot',\n",
       " 'to',\n",
       " 'school',\n",
       " '.',\n",
       " 'during',\n",
       " 'show',\n",
       " 'and',\n",
       " 'tell',\n",
       " ',',\n",
       " 'josh',\n",
       " \"'s\",\n",
       " 'parrot',\n",
       " 'said',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'word',\n",
       " '.',\n",
       " 'the',\n",
       " 'teacher',\n",
       " 'told',\n",
       " 'joshua',\n",
       " 'not',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'his',\n",
       " 'bird',\n",
       " 'again',\n",
       " '.',\n",
       " 'when',\n",
       " 'josh',\n",
       " 'got',\n",
       " 'home',\n",
       " ',',\n",
       " 'he',\n",
       " 'was',\n",
       " 'grounded',\n",
       " '.',\n",
       " 'hal',\n",
       " 'was',\n",
       " 'walking',\n",
       " 'his',\n",
       " 'dog',\n",
       " 'one',\n",
       " 'morning',\n",
       " '.',\n",
       " 'a',\n",
       " 'cat',\n",
       " 'ran',\n",
       " 'across',\n",
       " 'their',\n",
       " 'path',\n",
       " '.',\n",
       " 'hal',\n",
       " \"'s\",\n",
       " 'dog',\n",
       " 'strained',\n",
       " 'so',\n",
       " 'hard',\n",
       " ',',\n",
       " 'the',\n",
       " 'leash',\n",
       " 'broke',\n",
       " '!',\n",
       " 'he',\n",
       " 'chased',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'for',\n",
       " 'several',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'finally',\n",
       " 'hal',\n",
       " 'lured',\n",
       " 'him',\n",
       " 'back',\n",
       " 'to',\n",
       " 'his',\n",
       " 'side',\n",
       " '.',\n",
       " 'brenda',\n",
       " 'was',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'her',\n",
       " 'boyfriend',\n",
       " 'maxwell',\n",
       " '.',\n",
       " 'he',\n",
       " 'was',\n",
       " 'a',\n",
       " 'successful',\n",
       " 'artist',\n",
       " 'with',\n",
       " 'a',\n",
       " 'promising',\n",
       " 'future',\n",
       " '.',\n",
       " 'maxwell',\n",
       " 'told',\n",
       " 'brenda',\n",
       " 'he',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'her',\n",
       " '.',\n",
       " 'she',\n",
       " 'thought',\n",
       " 'he',\n",
       " \"'d\",\n",
       " 'propose',\n",
       " 'but',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'break',\n",
       " 'up',\n",
       " '.',\n",
       " 'brenda',\n",
       " 'walked',\n",
       " 'away',\n",
       " 'and',\n",
       " 'now',\n",
       " 'she',\n",
       " 'is',\n",
       " 'the',\n",
       " 'saddest',\n",
       " 'girl',\n",
       " 'out',\n",
       " 'of',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'yanice',\n",
       " 'opened',\n",
       " 'the',\n",
       " 'fridge',\n",
       " 'and',\n",
       " 'found',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'eat',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'there',\n",
       " 'were',\n",
       " 'leftovers',\n",
       " '.',\n",
       " 'she',\n",
       " 'mixed',\n",
       " 'it',\n",
       " 'up',\n",
       " 'in',\n",
       " 'an',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'make',\n",
       " 'lunch',\n",
       " '.',\n",
       " 'since',\n",
       " 'the',\n",
       " 'place',\n",
       " 'needed',\n",
       " 'meat',\n",
       " ',',\n",
       " 'she',\n",
       " 'also',\n",
       " 'fried',\n",
       " 'and',\n",
       " 'eggs',\n",
       " '.',\n",
       " 'she',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'enjoying',\n",
       " 'the',\n",
       " 'meal',\n",
       " '.',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'joe',\n",
       " 'sitting',\n",
       " 'in',\n",
       " 'lobby',\n",
       " 'today',\n",
       " '.',\n",
       " 'i',\n",
       " 'kept',\n",
       " 'him',\n",
       " 'company',\n",
       " ',',\n",
       " 'as',\n",
       " 'he',\n",
       " 'is',\n",
       " 'a',\n",
       " 'lonely',\n",
       " 'old',\n",
       " 'man',\n",
       " '.',\n",
       " 'he',\n",
       " 'told',\n",
       " 'me',\n",
       " 'he',\n",
       " 'had',\n",
       " 'just',\n",
       " 'listened',\n",
       " 'to',\n",
       " 'beethoven',\n",
       " \"'s\",\n",
       " 'ninth',\n",
       " '.',\n",
       " 'i',\n",
       " 'talked',\n",
       " 'to',\n",
       " 'him',\n",
       " 'for',\n",
       " 'an',\n",
       " 'hour',\n",
       " '.',\n",
       " 'i',\n",
       " 'left',\n",
       " 'him',\n",
       " 'in',\n",
       " 'the',\n",
       " 'lobby',\n",
       " 'and',\n",
       " 'told',\n",
       " 'him',\n",
       " 'i',\n",
       " 'would',\n",
       " 'see',\n",
       " 'him',\n",
       " 'soon',\n",
       " '.',\n",
       " 'twas',\n",
       " 'the',\n",
       " 'night',\n",
       " 'after',\n",
       " 'the',\n",
       " 'first',\n",
       " 'day',\n",
       " 'of',\n",
       " 'junior',\n",
       " 'high',\n",
       " '.',\n",
       " 'amy',\n",
       " 'and',\n",
       " 'her',\n",
       " 'friend',\n",
       " 'beth',\n",
       " 'were',\n",
       " 'on',\n",
       " 'the',\n",
       " 'phone',\n",
       " '.',\n",
       " 'they',\n",
       " 'had',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'to',\n",
       " 'catch',\n",
       " 'up',\n",
       " 'on',\n",
       " '.',\n",
       " 'amy',\n",
       " 'listened',\n",
       " 'patiently',\n",
       " 'as',\n",
       " 'beth',\n",
       " 'told',\n",
       " 'her',\n",
       " 'about',\n",
       " 'her',\n",
       " 'day',\n",
       " '.',\n",
       " 'she',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'go',\n",
       " '2nd',\n",
       " 'because',\n",
       " 'she',\n",
       " 'knew',\n",
       " 'hers',\n",
       " 'was',\n",
       " 'the',\n",
       " 'better',\n",
       " 'day',\n",
       " '.',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'of',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'who',\n",
       " 'won',\n",
       " 'the',\n",
       " 'lottery',\n",
       " '.',\n",
       " 'he',\n",
       " 'used',\n",
       " 'to',\n",
       " 'ride',\n",
       " 'lawn',\n",
       " 'mowers',\n",
       " '.',\n",
       " 'after',\n",
       " 'he',\n",
       " 'won',\n",
       " ',',\n",
       " 'he',\n",
       " 'went',\n",
       " 'on',\n",
       " 'to',\n",
       " 'using',\n",
       " 'drugs',\n",
       " '.',\n",
       " 'he',\n",
       " 'blew',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'money',\n",
       " '.',\n",
       " 'eventually',\n",
       " 'his',\n",
       " 'winnings',\n",
       " 'were',\n",
       " 'revoked',\n",
       " 'after',\n",
       " 'a',\n",
       " 'dui',\n",
       " '.',\n",
       " 'a',\n",
       " 'die',\n",
       " 'hard',\n",
       " 'shopper',\n",
       " 'was',\n",
       " 'waiting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'long',\n",
       " 'line',\n",
       " 'outside',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'miserably',\n",
       " 'cold',\n",
       " '.',\n",
       " 'the',\n",
       " 'shopper',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'homeless',\n",
       " 'man',\n",
       " 'shivering',\n",
       " 'in',\n",
       " 'the',\n",
       " 'alleyway',\n",
       " '.',\n",
       " 'he',\n",
       " 'gave',\n",
       " 'up',\n",
       " 'his',\n",
       " 'place',\n",
       " 'in',\n",
       " 'the',\n",
       " 'line',\n",
       " 'and',\n",
       " 'brought',\n",
       " 'a',\n",
       " 'gift',\n",
       " 'back',\n",
       " 'from',\n",
       " 'his',\n",
       " 'car',\n",
       " '.',\n",
       " 'the',\n",
       " 'shopper',\n",
       " 'gave',\n",
       " 'the',\n",
       " 'homeless',\n",
       " 'man',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'warm',\n",
       " 'blanket',\n",
       " '.',\n",
       " 'jeff',\n",
       " 'invited',\n",
       " 'his',\n",
       " 'friends',\n",
       " 'over',\n",
       " 'to',\n",
       " 'play',\n",
       " 'board',\n",
       " 'games',\n",
       " 'on',\n",
       " 'saturday',\n",
       " 'night',\n",
       " '.',\n",
       " 'they',\n",
       " 'arrived',\n",
       " 'at',\n",
       " 'his',\n",
       " 'house',\n",
       " 'early',\n",
       " 'that',\n",
       " 'evening',\n",
       " '.',\n",
       " 'the',\n",
       " 'six',\n",
       " 'of',\n",
       " 'them',\n",
       " 'sat',\n",
       " 'around',\n",
       " 'a',\n",
       " 'big',\n",
       " 'table',\n",
       " '.',\n",
       " 'they',\n",
       " 'took',\n",
       " 'turns',\n",
       " 'deciding',\n",
       " 'which',\n",
       " 'game',\n",
       " 'to',\n",
       " 'play',\n",
       " '.',\n",
       " 'they',\n",
       " 'spent',\n",
       " 'six',\n",
       " 'hours',\n",
       " 'playing',\n",
       " 'different',\n",
       " 'board',\n",
       " 'games',\n",
       " '.',\n",
       " 'chuck',\n",
       " 'reclined',\n",
       " 'on',\n",
       " 'the',\n",
       " 'back',\n",
       " 'porch',\n",
       " 'as',\n",
       " 'he',\n",
       " 'sipped',\n",
       " 'his',\n",
       " 'morning',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list_story = [item for sublist in train_stories['Tokenized_Story'].values for item in sublist ]\n",
    "\n",
    "flat_list_story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def token_count(token_seqs):\n",
    "    \n",
    "    cv = CountVectorizer(stop_words=None, analyzer='word', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None,\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|!|\\?|\\\"|\\'|\\.\")\n",
    "    cv_fit=cv.fit_transform(token_seqs)\n",
    "    word_list = cv.get_feature_names();\n",
    "    count_list = cv_fit.toarray().sum(axis=0)\n",
    "    z = dict(zip(word_list,count_list))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 13,\n",
       " '\"': 2,\n",
       " \"'\": 52,\n",
       " '.': 486,\n",
       " '1': 1,\n",
       " '10': 1,\n",
       " '100': 1,\n",
       " '10000': 1,\n",
       " '16th': 1,\n",
       " '18': 1,\n",
       " '1987': 1,\n",
       " '2': 1,\n",
       " '2013': 1,\n",
       " '2nd': 1,\n",
       " '300': 1,\n",
       " '31': 1,\n",
       " '50': 1,\n",
       " '?': 1,\n",
       " 'a': 154,\n",
       " 'able': 5,\n",
       " 'about': 12,\n",
       " 'accepted': 1,\n",
       " 'accidentally': 1,\n",
       " 'account': 1,\n",
       " 'across': 1,\n",
       " 'acted': 1,\n",
       " 'actually': 1,\n",
       " 'adds': 1,\n",
       " 'affinity': 1,\n",
       " 'after': 16,\n",
       " 'afternoon': 1,\n",
       " 'afterward': 1,\n",
       " 'again': 2,\n",
       " 'against': 1,\n",
       " 'ages': 1,\n",
       " 'agree': 1,\n",
       " 'agreed': 1,\n",
       " 'ahead': 1,\n",
       " 'aid': 1,\n",
       " 'airplanes': 1,\n",
       " 'aisle': 1,\n",
       " 'all': 16,\n",
       " 'alleyway': 1,\n",
       " 'allows': 1,\n",
       " 'alone': 1,\n",
       " 'along': 2,\n",
       " 'already': 2,\n",
       " 'alright': 1,\n",
       " 'also': 2,\n",
       " 'although': 1,\n",
       " 'always': 4,\n",
       " 'amazingly': 1,\n",
       " 'amber': 1,\n",
       " 'amy': 2,\n",
       " 'an': 8,\n",
       " 'and': 121,\n",
       " 'animal': 1,\n",
       " 'ann': 4,\n",
       " 'anna': 2,\n",
       " 'annoyed': 1,\n",
       " 'anticipated': 1,\n",
       " 'any': 2,\n",
       " 'anyone': 1,\n",
       " 'anything': 3,\n",
       " 'apartment': 1,\n",
       " 'apologize': 1,\n",
       " 'applauded': 1,\n",
       " 'april': 1,\n",
       " 'are': 4,\n",
       " 'area': 1,\n",
       " 'around': 5,\n",
       " 'arrived': 3,\n",
       " 'artist': 1,\n",
       " 'as': 15,\n",
       " 'ask': 1,\n",
       " 'asked': 4,\n",
       " 'asking': 2,\n",
       " 'astonishment': 1,\n",
       " 'at': 25,\n",
       " 'ate': 1,\n",
       " 'attempt': 1,\n",
       " 'attention': 1,\n",
       " 'audio': 1,\n",
       " 'audition': 1,\n",
       " 'auditioned': 1,\n",
       " 'auditions': 1,\n",
       " 'away': 6,\n",
       " 'baby': 4,\n",
       " 'back': 12,\n",
       " 'bacon': 2,\n",
       " 'bad': 5,\n",
       " 'badly': 1,\n",
       " 'bahamas': 1,\n",
       " 'bake': 1,\n",
       " 'ball': 4,\n",
       " 'ballerina': 1,\n",
       " 'band': 2,\n",
       " 'bank': 1,\n",
       " 'barged': 1,\n",
       " 'bath': 1,\n",
       " 'bathroom': 2,\n",
       " 'be': 10,\n",
       " 'beach': 4,\n",
       " 'beautiful': 1,\n",
       " 'became': 4,\n",
       " 'because': 2,\n",
       " 'becoming': 1,\n",
       " 'been': 7,\n",
       " 'beer': 1,\n",
       " 'beers': 1,\n",
       " 'beethoven': 1,\n",
       " 'began': 6,\n",
       " 'behind': 1,\n",
       " 'being': 4,\n",
       " 'believed': 1,\n",
       " 'belongings': 1,\n",
       " 'ben': 2,\n",
       " 'bent': 1,\n",
       " 'best': 3,\n",
       " 'beth': 2,\n",
       " 'better': 4,\n",
       " 'betty': 3,\n",
       " 'big': 1,\n",
       " 'bike': 4,\n",
       " 'billy': 3,\n",
       " 'bird': 1,\n",
       " 'birthday': 1,\n",
       " 'bit': 5,\n",
       " 'bite': 1,\n",
       " 'blanket': 1,\n",
       " 'blessed': 1,\n",
       " 'blew': 2,\n",
       " 'blue': 1,\n",
       " 'board': 2,\n",
       " 'boat': 1,\n",
       " 'bob': 6,\n",
       " 'bogart': 2,\n",
       " 'book': 1,\n",
       " 'booked': 1,\n",
       " 'both': 4,\n",
       " 'bought': 6,\n",
       " 'bounced': 1,\n",
       " 'bow': 1,\n",
       " 'bowl': 1,\n",
       " 'boxes': 2,\n",
       " 'boy': 4,\n",
       " 'boyfriend': 4,\n",
       " 'break': 1,\n",
       " 'breaking': 1,\n",
       " 'brenda': 3,\n",
       " 'bright': 2,\n",
       " 'bring': 2,\n",
       " 'broke': 3,\n",
       " 'broken': 2,\n",
       " 'brought': 3,\n",
       " 'bug': 1,\n",
       " 'bully': 1,\n",
       " 'bunch': 1,\n",
       " 'bunnies': 1,\n",
       " 'bunny': 1,\n",
       " 'burnt': 1,\n",
       " 'bursting': 1,\n",
       " 'bus': 10,\n",
       " 'but': 23,\n",
       " 'buy': 5,\n",
       " 'buying': 1,\n",
       " 'buys': 1,\n",
       " 'by': 6,\n",
       " 'ca': 1,\n",
       " 'cabin': 2,\n",
       " 'cade': 4,\n",
       " 'cafeteria': 1,\n",
       " 'cake': 1,\n",
       " 'california': 1,\n",
       " 'call': 1,\n",
       " 'called': 5,\n",
       " 'calling': 1,\n",
       " 'calls': 1,\n",
       " 'calmed': 1,\n",
       " 'came': 8,\n",
       " 'can': 1,\n",
       " 'candy': 2,\n",
       " 'car': 14,\n",
       " 'carrie': 3,\n",
       " 'cars': 1,\n",
       " 'cart': 1,\n",
       " 'cat': 2,\n",
       " 'catch': 1,\n",
       " 'catholic': 1,\n",
       " 'caught': 3,\n",
       " 'cave': 2,\n",
       " 'chair': 1,\n",
       " 'change': 4,\n",
       " 'chant': 1,\n",
       " 'chanting': 1,\n",
       " 'charge': 1,\n",
       " 'charged': 1,\n",
       " 'chased': 1,\n",
       " 'cheered': 1,\n",
       " 'chicago': 1,\n",
       " 'chicken': 2,\n",
       " 'child': 1,\n",
       " 'chip': 4,\n",
       " 'christmas': 3,\n",
       " 'chuck': 1,\n",
       " 'clashed': 2,\n",
       " 'class': 6,\n",
       " 'clue': 1,\n",
       " 'coach': 2,\n",
       " 'coffee': 6,\n",
       " 'coffeeshop': 1,\n",
       " 'cold': 3,\n",
       " 'college': 1,\n",
       " 'come': 3,\n",
       " 'coming': 1,\n",
       " 'community': 1,\n",
       " 'company': 1,\n",
       " 'competed': 1,\n",
       " 'complain': 1,\n",
       " 'complained': 2,\n",
       " 'completely': 1,\n",
       " 'condo': 2,\n",
       " 'confessed': 1,\n",
       " 'confessing': 1,\n",
       " 'confessional': 2,\n",
       " 'contain': 1,\n",
       " 'contest': 1,\n",
       " 'continued': 1,\n",
       " 'convinced': 2,\n",
       " 'cook': 2,\n",
       " 'cooks': 1,\n",
       " 'couch': 1,\n",
       " 'could': 12,\n",
       " 'counter': 1,\n",
       " 'courage': 1,\n",
       " 'court': 1,\n",
       " 'covered': 2,\n",
       " 'coworker': 1,\n",
       " 'craigslist': 1,\n",
       " 'crashed': 1,\n",
       " 'craving': 1,\n",
       " 'cream': 5,\n",
       " 'crush': 3,\n",
       " 'crying': 2,\n",
       " 'cup': 2,\n",
       " 'customer': 1,\n",
       " 'customers': 1,\n",
       " 'cut': 2,\n",
       " 'cute': 1,\n",
       " 'cuts': 1,\n",
       " 'd': 2,\n",
       " 'dad': 7,\n",
       " 'dan': 12,\n",
       " 'dance': 2,\n",
       " 'dark': 1,\n",
       " 'date': 2,\n",
       " 'dating': 1,\n",
       " 'dave': 3,\n",
       " 'day': 27,\n",
       " 'dead': 1,\n",
       " 'deaf': 1,\n",
       " 'decided': 19,\n",
       " 'deciding': 1,\n",
       " 'decor': 1,\n",
       " 'deep': 2,\n",
       " 'deeper': 1,\n",
       " 'deer': 2,\n",
       " 'delicious': 2,\n",
       " 'demurred': 1,\n",
       " 'descended': 1,\n",
       " 'deserve': 1,\n",
       " 'despite': 1,\n",
       " 'determined': 1,\n",
       " 'diagram': 1,\n",
       " 'diana': 1,\n",
       " 'did': 19,\n",
       " 'die': 1,\n",
       " 'died': 1,\n",
       " 'diet': 1,\n",
       " 'different': 1,\n",
       " 'diner': 1,\n",
       " 'dinner': 1,\n",
       " 'dip': 4,\n",
       " 'dipped': 1,\n",
       " 'directions': 1,\n",
       " 'disagreed': 1,\n",
       " 'disapproved': 1,\n",
       " 'discount': 1,\n",
       " 'discovered': 1,\n",
       " 'dishes': 1,\n",
       " 'distracted': 1,\n",
       " 'divorce': 1,\n",
       " 'do': 2,\n",
       " 'doctor': 1,\n",
       " 'doctors': 1,\n",
       " 'dog': 6,\n",
       " 'doing': 1,\n",
       " 'dollars': 2,\n",
       " 'done': 2,\n",
       " 'door': 2,\n",
       " 'doorbell': 1,\n",
       " 'doorway': 1,\n",
       " 'double': 4,\n",
       " 'down': 8,\n",
       " 'dread': 1,\n",
       " 'dream': 2,\n",
       " 'dreams': 1,\n",
       " 'drink': 1,\n",
       " 'drinking': 2,\n",
       " 'drinks': 1,\n",
       " 'drive': 1,\n",
       " 'driver': 2,\n",
       " 'driving': 2,\n",
       " 'dropped': 1,\n",
       " 'drove': 6,\n",
       " 'drugs': 1,\n",
       " 'drunk': 1,\n",
       " 'dui': 1,\n",
       " 'during': 4,\n",
       " 'each': 4,\n",
       " 'eagerly': 1,\n",
       " 'early': 2,\n",
       " 'easier': 1,\n",
       " 'eat': 3,\n",
       " 'eating': 1,\n",
       " 'edge': 1,\n",
       " 'egg': 3,\n",
       " 'eggs': 3,\n",
       " 'either': 1,\n",
       " 'elated': 1,\n",
       " 'electronic': 1,\n",
       " 'elephant': 1,\n",
       " 'elevator': 1,\n",
       " 'elizabeth': 1,\n",
       " 'elliott': 2,\n",
       " 'else': 1,\n",
       " 'encourage': 1,\n",
       " 'encouraged': 1,\n",
       " 'end': 3,\n",
       " 'ended': 6,\n",
       " 'engine': 1,\n",
       " 'enjoy': 1,\n",
       " 'enjoyed': 2,\n",
       " 'enjoying': 2,\n",
       " 'enough': 3,\n",
       " 'entered': 2,\n",
       " 'epcot': 1,\n",
       " 'equipment': 1,\n",
       " 'erica': 1,\n",
       " 'errands': 1,\n",
       " 'eruption': 1,\n",
       " 'escape': 1,\n",
       " 'even': 3,\n",
       " 'evening': 2,\n",
       " 'eventually': 3,\n",
       " 'ever': 2,\n",
       " 'every': 2,\n",
       " 'everyday': 1,\n",
       " 'everyone': 4,\n",
       " 'everything': 2,\n",
       " 'everywhere': 2,\n",
       " 'excited': 5,\n",
       " 'excitement': 1,\n",
       " 'expecting': 1,\n",
       " 'exploded': 1,\n",
       " 'extended': 1,\n",
       " 'exterminator': 2,\n",
       " 'extra': 1,\n",
       " 'eyes': 1,\n",
       " 'fabric': 1,\n",
       " 'face': 1,\n",
       " 'facials': 1,\n",
       " 'family': 5,\n",
       " 'famous': 1,\n",
       " 'fantasized': 1,\n",
       " 'far': 3,\n",
       " 'farm': 3,\n",
       " 'farther': 1,\n",
       " 'fast': 1,\n",
       " 'father': 2,\n",
       " 'favorite': 2,\n",
       " 'feared': 1,\n",
       " 'fed': 1,\n",
       " 'feeding': 1,\n",
       " 'feel': 2,\n",
       " 'feeling': 1,\n",
       " 'fees': 1,\n",
       " 'feet': 2,\n",
       " 'fell': 4,\n",
       " 'felt': 6,\n",
       " 'few': 2,\n",
       " 'field': 2,\n",
       " 'fight': 1,\n",
       " 'fighting': 2,\n",
       " 'fill': 1,\n",
       " 'filled': 1,\n",
       " 'final': 1,\n",
       " 'finally': 4,\n",
       " 'find': 1,\n",
       " 'finding': 1,\n",
       " 'finger': 1,\n",
       " 'finish': 2,\n",
       " 'finishing': 1,\n",
       " 'fire': 2,\n",
       " 'firefighters': 1,\n",
       " 'first': 6,\n",
       " 'fishing': 1,\n",
       " 'five': 1,\n",
       " 'fix': 1,\n",
       " 'fixed': 1,\n",
       " 'flew': 1,\n",
       " 'flip': 1,\n",
       " 'flirting': 1,\n",
       " 'flood': 1,\n",
       " 'floor': 1,\n",
       " 'flying': 1,\n",
       " 'fond': 1,\n",
       " 'food': 5,\n",
       " 'fools': 1,\n",
       " 'foot': 1,\n",
       " 'footing': 1,\n",
       " 'for': 46,\n",
       " 'forgot': 3,\n",
       " 'forming': 1,\n",
       " 'found': 13,\n",
       " 'frame': 1,\n",
       " 'frank': 7,\n",
       " 'frankie': 1,\n",
       " 'free': 1,\n",
       " 'fresh': 2,\n",
       " 'fridge': 2,\n",
       " 'fried': 1,\n",
       " 'friend': 3,\n",
       " 'friends': 6,\n",
       " 'from': 13,\n",
       " 'frustrated': 1,\n",
       " 'full': 3,\n",
       " 'fun': 2,\n",
       " 'functioning': 1,\n",
       " 'future': 1,\n",
       " 'gained': 1,\n",
       " 'game': 1,\n",
       " 'games': 2,\n",
       " 'garden': 1,\n",
       " 'gary': 4,\n",
       " 'gas': 4,\n",
       " 'gash': 1,\n",
       " 'gathered': 1,\n",
       " 'gave': 3,\n",
       " 'get': 9,\n",
       " 'gets': 2,\n",
       " 'getting': 1,\n",
       " 'gift': 1,\n",
       " 'gifts': 1,\n",
       " 'gig': 1,\n",
       " 'gina': 5,\n",
       " 'girl': 2,\n",
       " 'girlfriend': 1,\n",
       " 'girls': 1,\n",
       " 'given': 1,\n",
       " 'glad': 2,\n",
       " 'go': 18,\n",
       " 'goal': 2,\n",
       " 'goes': 1,\n",
       " 'going': 5,\n",
       " 'golf': 1,\n",
       " 'gone': 1,\n",
       " 'good': 3,\n",
       " 'got': 20,\n",
       " 'grabbed': 1,\n",
       " 'graduated': 1,\n",
       " 'grandfather': 1,\n",
       " 'grass': 2,\n",
       " 'grateful': 1,\n",
       " 'great': 3,\n",
       " 'green': 4,\n",
       " 'greg': 1,\n",
       " 'grew': 1,\n",
       " 'grind': 1,\n",
       " 'grocery': 1,\n",
       " 'ground': 2,\n",
       " 'grounded': 1,\n",
       " 'grow': 3,\n",
       " 'guitar': 3,\n",
       " 'guys': 1,\n",
       " 'gym': 2,\n",
       " 'had': 58,\n",
       " 'hair': 2,\n",
       " 'haircut': 1,\n",
       " 'hal': 3,\n",
       " 'half': 2,\n",
       " 'hallway': 1,\n",
       " 'hamburgers': 1,\n",
       " 'hand': 1,\n",
       " 'hands': 1,\n",
       " 'hang': 1,\n",
       " 'happened': 1,\n",
       " 'happily': 2,\n",
       " 'happy': 1,\n",
       " 'hard': 3,\n",
       " 'has': 4,\n",
       " 'hauled': 1,\n",
       " 'have': 8,\n",
       " 'hawkins': 1,\n",
       " 'he': 115,\n",
       " 'head': 1,\n",
       " 'headed': 1,\n",
       " 'heading': 2,\n",
       " 'hear': 2,\n",
       " 'heard': 3,\n",
       " 'hearing': 1,\n",
       " 'heart': 1,\n",
       " 'heat': 1,\n",
       " 'help': 6,\n",
       " 'helped': 2,\n",
       " 'helping': 1,\n",
       " 'her': 98,\n",
       " 'herbs': 1,\n",
       " 'hers': 1,\n",
       " 'herself': 2,\n",
       " 'hide': 1,\n",
       " 'hideous': 1,\n",
       " 'hiding': 1,\n",
       " 'high': 3,\n",
       " 'highway': 2,\n",
       " 'hill': 1,\n",
       " 'him': 25,\n",
       " 'his': 66,\n",
       " 'hit': 1,\n",
       " 'hits': 1,\n",
       " 'hold': 1,\n",
       " 'home': 20,\n",
       " 'homeless': 2,\n",
       " 'homer': 2,\n",
       " 'hood': 1,\n",
       " 'hop': 1,\n",
       " 'hope': 1,\n",
       " 'hopes': 1,\n",
       " 'hoping': 1,\n",
       " 'hospital': 1,\n",
       " 'hot': 5,\n",
       " 'hour': 2,\n",
       " 'hours': 4,\n",
       " 'house': 6,\n",
       " 'how': 7,\n",
       " 'however': 2,\n",
       " 'huge': 3,\n",
       " 'hundreds': 2,\n",
       " 'hungry': 1,\n",
       " 'hunt': 2,\n",
       " 'husband': 1,\n",
       " 'i': 36,\n",
       " 'ice': 4,\n",
       " 'iced': 1,\n",
       " 'if': 2,\n",
       " 'implanted': 1,\n",
       " 'impression': 1,\n",
       " 'in': 48,\n",
       " 'inch': 1,\n",
       " 'ingredients': 1,\n",
       " 'inside': 2,\n",
       " 'instead': 2,\n",
       " 'instrument': 1,\n",
       " 'internationally': 1,\n",
       " 'intervened': 1,\n",
       " 'into': 15,\n",
       " 'invite': 1,\n",
       " 'invited': 2,\n",
       " 'is': 10,\n",
       " 'it': 57,\n",
       " 'james': 2,\n",
       " 'jane': 5,\n",
       " 'jeff': 6,\n",
       " 'jen': 4,\n",
       " 'jenna': 2,\n",
       " 'jerry': 2,\n",
       " 'jessica': 1,\n",
       " 'jill': 1,\n",
       " 'jimmy': 2,\n",
       " 'joan': 3,\n",
       " 'joanie': 2,\n",
       " 'job': 4,\n",
       " 'joe': 1,\n",
       " 'jogging': 1,\n",
       " 'john': 5,\n",
       " 'join': 1,\n",
       " 'joined': 1,\n",
       " 'josh': 3,\n",
       " 'joshua': 1,\n",
       " 'joy': 1,\n",
       " 'jumped': 1,\n",
       " 'junior': 1,\n",
       " 'just': 6,\n",
       " 'keep': 1,\n",
       " 'kelly': 2,\n",
       " 'kept': 2,\n",
       " 'keys': 1,\n",
       " 'kids': 3,\n",
       " 'killed': 1,\n",
       " 'kim': 2,\n",
       " 'kind': 1,\n",
       " 'kitchen': 4,\n",
       " 'kneeled': 1,\n",
       " 'knew': 5,\n",
       " 'knocked': 1,\n",
       " 'know': 1,\n",
       " 'labored': 1,\n",
       " 'lake': 2,\n",
       " 'landed': 2,\n",
       " 'large': 2,\n",
       " 'last': 5,\n",
       " 'laugh': 1,\n",
       " 'laughing': 1,\n",
       " 'laura': 2,\n",
       " 'lava': 1,\n",
       " 'lawn': 1,\n",
       " 'laws': 2,\n",
       " 'lead': 1,\n",
       " 'league': 1,\n",
       " 'learned': 4,\n",
       " 'leash': 1,\n",
       " 'left': 2,\n",
       " 'leftovers': 1,\n",
       " 'leg': 1,\n",
       " 'lesson': 1,\n",
       " 'lessons': 4,\n",
       " 'life': 2,\n",
       " 'lifeguard': 1,\n",
       " 'lifting': 1,\n",
       " 'like': 10,\n",
       " 'liked': 2,\n",
       " 'likes': 1,\n",
       " 'lily': 1,\n",
       " 'limits': 1,\n",
       " 'line': 2,\n",
       " 'lip': 1,\n",
       " 'lisa': 2,\n",
       " 'listened': 4,\n",
       " 'little': 2,\n",
       " 'lived': 2,\n",
       " 'living': 2,\n",
       " 'lobby': 2,\n",
       " 'local': 3,\n",
       " 'locker': 1,\n",
       " 'logs': 1,\n",
       " 'lonely': 1,\n",
       " 'long': 5,\n",
       " 'longer': 2,\n",
       " 'looked': 8,\n",
       " 'looking': 6,\n",
       " 'lost': 3,\n",
       " 'lot': 6,\n",
       " 'lots': 1,\n",
       " 'lottery': 1,\n",
       " 'loudly': 1,\n",
       " 'louisa': 1,\n",
       " 'love': 2,\n",
       " 'loved': 5,\n",
       " 'luckily': 1,\n",
       " 'lunch': 4,\n",
       " 'lured': 1,\n",
       " 'lutheran': 1,\n",
       " 'machine': 1,\n",
       " 'made': 12,\n",
       " 'make': 8,\n",
       " 'makes': 1,\n",
       " 'making': 3,\n",
       " 'mall': 1,\n",
       " 'man': 11,\n",
       " 'managed': 1,\n",
       " 'mangy': 1,\n",
       " 'marching': 1,\n",
       " 'mark': 6,\n",
       " 'marks': 1,\n",
       " 'marriage': 1,\n",
       " 'married': 1,\n",
       " 'marry': 1,\n",
       " 'mary': 1,\n",
       " 'massages': 1,\n",
       " 'match': 1,\n",
       " 'material': 1,\n",
       " 'maxwell': 2,\n",
       " 'me': 7,\n",
       " 'meal': 1,\n",
       " 'mean': 2,\n",
       " 'meat': 1,\n",
       " 'mechanic': 1,\n",
       " 'medal': 1,\n",
       " 'medium': 1,\n",
       " 'melted': 1,\n",
       " 'membership': 1,\n",
       " 'mess': 1,\n",
       " 'messed': 1,\n",
       " 'met': 3,\n",
       " 'mia': 2,\n",
       " 'mice': 1,\n",
       " 'microwave': 2,\n",
       " 'mike': 1,\n",
       " 'mile': 1,\n",
       " 'mint': 4,\n",
       " 'minute': 1,\n",
       " 'minutes': 1,\n",
       " 'miserably': 1,\n",
       " 'missed': 2,\n",
       " 'missing': 1,\n",
       " 'mistake': 1,\n",
       " 'mixed': 1,\n",
       " 'mom': 10,\n",
       " 'moment': 1,\n",
       " 'money': 6,\n",
       " 'months': 2,\n",
       " 'more': 5,\n",
       " 'morgan': 2,\n",
       " 'morning': 3,\n",
       " 'mother': 2,\n",
       " 'motivation': 1,\n",
       " 'move': 1,\n",
       " 'moved': 1,\n",
       " 'movie': 8,\n",
       " 'movies': 1,\n",
       " 'moving': 4,\n",
       " 'mowers': 1,\n",
       " 'mr': 1,\n",
       " 'much': 5,\n",
       " 'music': 1,\n",
       " 'musician': 1,\n",
       " 'my': 13,\n",
       " 'n': 27,\n",
       " 'named': 1,\n",
       " 'natalie': 1,\n",
       " 'nate': 2,\n",
       " 'nearest': 1,\n",
       " 'need': 1,\n",
       " 'needed': 7,\n",
       " 'neighbor': 1,\n",
       " 'nerves': 1,\n",
       " 'nervous': 2,\n",
       " 'never': 2,\n",
       " 'new': 14,\n",
       " 'news': 1,\n",
       " 'newspaper': 1,\n",
       " 'next': 3,\n",
       " 'nice': 4,\n",
       " 'night': 8,\n",
       " 'ninth': 1,\n",
       " 'no': 9,\n",
       " 'noise': 1,\n",
       " 'normal': 1,\n",
       " 'nostalgic': 1,\n",
       " 'not': 9,\n",
       " 'nothing': 3,\n",
       " 'noticed': 2,\n",
       " 'now': 4,\n",
       " 'number': 1,\n",
       " 'nurse': 1,\n",
       " 'oatmeal': 1,\n",
       " 'obnoxious': 1,\n",
       " 'obsessed': 1,\n",
       " 'obviously': 1,\n",
       " 'of': 44,\n",
       " 'off': 6,\n",
       " 'offered': 2,\n",
       " 'officer': 1,\n",
       " 'old': 4,\n",
       " 'olivia': 1,\n",
       " 'on': 60,\n",
       " 'once': 1,\n",
       " 'one': 21,\n",
       " 'ones': 1,\n",
       " 'online': 2,\n",
       " 'only': 2,\n",
       " 'onto': 1,\n",
       " 'opened': 2,\n",
       " 'opening': 2,\n",
       " 'opportunity': 1,\n",
       " 'other': 5,\n",
       " 'our': 2,\n",
       " 'out': 20,\n",
       " 'outside': 5,\n",
       " 'oven': 1,\n",
       " 'over': 6,\n",
       " 'overdraft': 1,\n",
       " 'overdrafted': 1,\n",
       " 'overweight': 2,\n",
       " 'owe': 1,\n",
       " 'own': 2,\n",
       " 'owner': 1,\n",
       " 'packed': 2,\n",
       " 'paid': 2,\n",
       " 'pain': 1,\n",
       " 'parents': 7,\n",
       " 'parish': 2,\n",
       " 'park': 4,\n",
       " 'parlor': 1,\n",
       " 'parrot': 3,\n",
       " 'part': 2,\n",
       " 'partner': 1,\n",
       " 'party': 1,\n",
       " 'passed': 1,\n",
       " 'past': 1,\n",
       " 'pasta': 2,\n",
       " 'path': 1,\n",
       " 'patiently': 2,\n",
       " 'pay': 2,\n",
       " 'paying': 1,\n",
       " 'payment': 1,\n",
       " 'peace': 1,\n",
       " 'pee': 2,\n",
       " 'people': 2,\n",
       " 'pet': 2,\n",
       " 'peter': 3,\n",
       " 'petrified': 1,\n",
       " 'phone': 2,\n",
       " 'phrases': 1,\n",
       " 'pick': 2,\n",
       " 'picked': 2,\n",
       " 'picking': 1,\n",
       " 'pictures': 1,\n",
       " 'pie': 2,\n",
       " 'pig': 3,\n",
       " 'pilot': 1,\n",
       " 'place': 3,\n",
       " 'plan': 1,\n",
       " 'planning': 1,\n",
       " 'plants': 1,\n",
       " 'play': 6,\n",
       " 'played': 6,\n",
       " 'player': 1,\n",
       " 'playing': 2,\n",
       " 'pocket': 1,\n",
       " 'point': 1,\n",
       " 'polarized': 1,\n",
       " 'police': 1,\n",
       " 'pool': 1,\n",
       " 'popcorn': 1,\n",
       " 'popular': 1,\n",
       " 'porch': 3,\n",
       " 'positive': 1,\n",
       " 'posted': 1,\n",
       " 'pot': 3,\n",
       " 'pounds': 1,\n",
       " 'pour': 1,\n",
       " 'pouring': 1,\n",
       " 'powered': 1,\n",
       " 'practiced': 1,\n",
       " 'pranks': 1,\n",
       " 'prayed': 1,\n",
       " 'preschoolers': 1,\n",
       " 'presenting': 1,\n",
       " 'pressed': 1,\n",
       " 'pretended': 1,\n",
       " 'priest': 3,\n",
       " 'producers': 1,\n",
       " 'products': 1,\n",
       " 'promising': 1,\n",
       " 'properly': 1,\n",
       " 'propose': 3,\n",
       " 'proud': 1,\n",
       " 'pull': 1,\n",
       " 'punch': 1,\n",
       " 'puppy': 1,\n",
       " 'purchases': 1,\n",
       " 'put': 4,\n",
       " 'puts': 1,\n",
       " 'putt': 1,\n",
       " 'questions': 1,\n",
       " 'quickly': 2,\n",
       " 'quite': 1,\n",
       " 'quoted': 1,\n",
       " 'rain': 2,\n",
       " 'rained': 1,\n",
       " 'raining': 1,\n",
       " 'rainy': 1,\n",
       " 'ran': 2,\n",
       " 'ranking': 1,\n",
       " 'rats': 1,\n",
       " 'react': 1,\n",
       " 'ready': 2,\n",
       " 'realized': 9,\n",
       " 'really': 1,\n",
       " 'received': 1,\n",
       " 'reclined': 1,\n",
       " 'recliner': 2,\n",
       " 'red': 1,\n",
       " 'refreshing': 1,\n",
       " 'refused': 1,\n",
       " 'regularly': 1,\n",
       " 'rehearsed': 1,\n",
       " 'rejection': 1,\n",
       " 'relax': 1,\n",
       " 'relaxing': 1,\n",
       " 'relief': 1,\n",
       " 'religion': 1,\n",
       " 'remembered': 2,\n",
       " 'replied': 1,\n",
       " 'responds': 1,\n",
       " 'rest': 1,\n",
       " 'restaurant': 1,\n",
       " 'results': 1,\n",
       " 'reupholstered': 1,\n",
       " 'reverse': 1,\n",
       " 'revoked': 1,\n",
       " 'rex': 3,\n",
       " 'ride': 3,\n",
       " 'rides': 1,\n",
       " 'right': 2,\n",
       " 'ring': 1,\n",
       " 'risk': 1,\n",
       " 'rival': 1,\n",
       " 'road': 1,\n",
       " 'rock': 1,\n",
       " 'rode': 1,\n",
       " 'roof': 1,\n",
       " 'room': 1,\n",
       " 'rosie': 1,\n",
       " 'rude': 1,\n",
       " 'run': 1,\n",
       " 'running': 1,\n",
       " 'rushed': 1,\n",
       " 'ryder': 3,\n",
       " 's': 22,\n",
       " 'sad': 1,\n",
       " 'saddest': 1,\n",
       " 'sadie': 1,\n",
       " 'safely': 1,\n",
       " 'said': 2,\n",
       " 'salesman': 1,\n",
       " 'sally': 4,\n",
       " 'samantha': 3,\n",
       " 'same': 2,\n",
       " 'sand': 2,\n",
       " 'sandy': 3,\n",
       " 'sapphire': 1,\n",
       " 'sarah': 1,\n",
       " 'sat': 5,\n",
       " 'satisfied': 1,\n",
       " 'saturday': 1,\n",
       " 'sauce': 1,\n",
       " 'sauna': 1,\n",
       " 'save': 1,\n",
       " 'saw': 11,\n",
       " 'say': 1,\n",
       " 'saying': 1,\n",
       " 'says': 1,\n",
       " 'scared': 1,\n",
       " 'scenery': 1,\n",
       " 'school': 8,\n",
       " 'science': 1,\n",
       " 'score': 1,\n",
       " 'scored': 1,\n",
       " 'scratching': 1,\n",
       " 'scrawny': 1,\n",
       " 'screen': 1,\n",
       " 'searched': 3,\n",
       " 'second': 1,\n",
       " 'secret': 1,\n",
       " 'see': 4,\n",
       " 'seeing': 3,\n",
       " 'seek': 1,\n",
       " 'seen': 1,\n",
       " 'sees': 1,\n",
       " 'self': 1,\n",
       " 'seller': 1,\n",
       " 'selling': 1,\n",
       " 'sells': 1,\n",
       " 'set': 1,\n",
       " 'settings': 1,\n",
       " 'several': 1,\n",
       " 'shannon': 1,\n",
       " 'shark': 2,\n",
       " 'sharpener': 1,\n",
       " 'she': 124,\n",
       " 'shed': 1,\n",
       " 'shiny': 1,\n",
       " 'ship': 1,\n",
       " 'shivering': 1,\n",
       " 'shoes': 1,\n",
       " 'shopper': 3,\n",
       " 'shopping': 1,\n",
       " 'shore': 1,\n",
       " 'short': 3,\n",
       " 'shortcomings': 1,\n",
       " 'shortly': 1,\n",
       " 'should': 2,\n",
       " 'shouted': 1,\n",
       " 'show': 1,\n",
       " 'showed': 1,\n",
       " 'shower': 1,\n",
       " 'shredded': 1,\n",
       " 'siblings': 1,\n",
       " 'sick': 1,\n",
       " 'side': 1,\n",
       " 'sides': 1,\n",
       " 'signed': 1,\n",
       " 'since': 5,\n",
       " 'sipped': 1,\n",
       " 'sister': 1,\n",
       " 'sit': 3,\n",
       " 'sitting': 2,\n",
       " 'six': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = token_count(flat_list_story)\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have this :\n",
    "````\n",
    "{'hello': 1,\n",
    " ',': 2,\n",
    " 'my': 1,\n",
    " 'name': 1,\n",
    " 'is': 3,\n",
    " 'ludo': 1,\n",
    " '.': 4,\n",
    " 'i': 2,\n",
    " 'learn': 1,\n",
    " 'deep': 1,\n",
    " 'learning': 1,\n",
    " 'and': 1,\n",
    " 'like': 1,\n",
    " 'it': 1,\n",
    " '!': 1,\n",
    " 'this': 1,\n",
    " 'the': 1,\n",
    " 'second': 1,\n",
    " 'story': 1,\n",
    " 'there': 1,\n",
    " 'no': 2,\n",
    " 'beginning': 1,\n",
    " 'end': 1}\n",
    " ````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dan': 12}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((key,value) for key, value in z.items() if key == 'dan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's make sure your operation is working fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're going to create a function that will make up our lexicon.\n",
    "\n",
    "We will therefore create a function that will make our lexicon with two parameters. \n",
    "1. ``token_seqs`` which will be a list containing the stories.\n",
    "2. ``min_freq`` which will determine how often it should appear in the story. We will set this default setting to 1, but we may need to change it later.  \n",
    "\n",
    "It is interesting to know the number of times the word appears in the text. But contrary to what one might think, just because they often appear does not mean they are important. It is often the opposite. Words such as \"and\", \"I\", \"the\", etc ... will often be repeated but are not necessarily very important and give little information\n",
    "\n",
    "Then the function will return a dictionary with a value that starts with two because element 0 is reserved for blank spaces and element 1 for words previously unknown.... \n",
    "\n",
    "**Exercise :** \n",
    "1. Create a token_counts variable. Determine the number of times a word appears in ``token_seqs``. Use the function previously created\n",
    "2. Create a lexicon variable. Then, assign each word to a numerical index. Filter words that occur less than min_freq times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Count tokens (words) in texts and add them to the lexicon'''\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 2. 0 is reserved for padding, and 1 for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON SAMPLE (1274 total items):\n",
      "{'dan': 2, \"'s\": 3, 'parents': 4, 'were': 5, 'overweight': 6, '.': 7, 'was': 8, 'as': 9, 'well': 10, 'the': 11, 'doctors': 12, 'told': 13, 'his': 14, 'it': 15, 'unhealthy': 16, 'understood': 17, 'and': 18, 'decided': 19, 'to': 20, 'make': 21}\n",
      "{'dan': 2, \"'s\": 3, 'parents': 4, 'were': 5, 'overweight': 6, '.': 7, 'was': 8, 'as': 9, 'well': 10, 'the': 11, 'doctors': 12, 'told': 13, 'his': 14, 'it': 15, 'unhealthy': 16, 'understood': 17, 'and': 18, 'decided': 19, 'to': 20, 'make': 21, 'a': 22, 'change': 23, 'they': 24, 'got': 25, 'themselves': 26, 'on': 27, 'diet': 28, 'carrie': 29, 'had': 30, 'just': 31, 'learned': 32, 'how': 33, 'ride': 34, 'bike': 35, 'she': 36, 'did': 37, \"n't\": 38, 'have': 39, 'of': 40, 'her': 41, 'own': 42, 'would': 43, 'sneak': 44, 'rides': 45, 'sister': 46, 'nervous': 47, 'hill': 48, 'crashed': 49, 'into': 50, 'wall': 51, 'frame': 52, 'bent': 53, 'deep': 54, 'gash': 55, 'leg': 56, 'morgan': 57, 'enjoyed': 58, 'long': 59, 'walks': 60, 'beach': 61, 'boyfriend': 62, 'go': 63, 'for': 64, 'walk': 65, 'after': 66, 'walking': 67, 'over': 68, 'mile': 69, ',': 70, 'something': 71, 'happened': 72, 'propose': 73, 'upset': 74, 'he': 75, 'first': 76, 'jane': 77, 'working': 78, 'at': 79, 'diner': 80, 'suddenly': 81, 'customer': 82, 'barged': 83, 'up': 84, 'counter': 85, 'began': 86, 'yelling': 87, 'about': 88, 'food': 89, 'taking': 90, 'know': 91, 'react': 92, 'luckily': 93, 'coworker': 94, 'intervened': 95, 'calmed': 96, 'man': 97, 'down': 98, 'i': 99, 'talking': 100, 'my': 101, 'crush': 102, 'today': 103, 'continued': 104, 'complain': 105, 'guys': 106, 'flirting': 107, 'with': 108, 'agree': 109, 'what': 110, 'says': 111, 'listened': 112, 'patiently': 113, 'home': 114, 'text': 115, 'from': 116, 'asked': 117, 'if': 118, 'we': 119, 'can': 120, 'hang': 121, 'out': 122, 'tomorrow': 123, 'frank': 124, 'been': 125, 'drinking': 126, 'beer': 127, 'call': 128, 'girlfriend': 129, 'asking': 130, 'where': 131, 'realized': 132, 'date': 133, 'that': 134, 'night': 135, 'since': 136, 'already': 137, 'bit': 138, 'drunk': 139, 'could': 140, 'not': 141, 'drive': 142, 'spent': 143, 'rest': 144, 'more': 145, 'beers': 146, 'dave': 147, 'in': 148, 'bahamas': 149, 'vacation': 150, 'snorkeling': 151, 'second': 152, 'day': 153, 'while': 154, 'saw': 155, 'cave': 156, 'ahead': 157, 'went': 158, 'terrified': 159, 'when': 160, 'found': 161, 'shark': 162, '!': 163, 'swam': 164, 'away': 165, 'fast': 166, 'but': 167, 'caught': 168, 'ate': 169, 'sunny': 170, 'going': 171, 'stepped': 172, 'car': 173, 'forgot': 174, 'quite': 175, 'sunglasses': 176, 'back': 177, 'heading': 178, 'towards': 179, 'mall': 180, 'some': 181, 'headed': 182, 'sally': 183, 'happy': 184, 'widowed': 185, 'mom': 186, 'new': 187, 'discovered': 188, 'siblings': 189, 'feel': 190, 'same': 191, 'flew': 192, 'visit': 193, 'husband': 194, 'although': 195, 'obviously': 196, 'love': 197, 'nothing': 198, 'like': 199, 'dad': 200, 'wondered': 201, \"'\": 202, 'marriage': 203, 'hit': 204, 'golf': 205, 'ball': 206, 'watched': 207, 'bounced': 208, 'grass': 209, 'sand': 210, 'trap': 211, 'pretended': 212, 'actually': 213, 'landed': 214, 'green': 215, 'friends': 216, 'paying': 217, 'attention': 218, 'so': 219, 'believed': 220, 'him': 221, 'snuck': 222, 'made': 223, 'putt': 224, '10': 225, 'feet': 226, 'josh': 227, 'parrot': 228, 'talked': 229, 'brought': 230, 'school': 231, 'during': 232, 'show': 233, 'tell': 234, 'said': 235, 'bad': 236, 'word': 237, 'teacher': 238, 'joshua': 239, 'bring': 240, 'bird': 241, 'again': 242, 'grounded': 243, 'hal': 244, 'dog': 245, 'one': 246, 'morning': 247, 'cat': 248, 'ran': 249, 'across': 250, 'their': 251, 'path': 252, 'strained': 253, 'hard': 254, 'leash': 255, 'broke': 256, 'chased': 257, 'several': 258, 'minutes': 259, 'finally': 260, 'lured': 261, 'side': 262, 'brenda': 263, 'maxwell': 264, 'successful': 265, 'artist': 266, 'promising': 267, 'future': 268, 'needed': 269, 'talk': 270, 'thought': 271, \"'d\": 272, 'wanted': 273, 'break': 274, 'walked': 275, 'now': 276, 'is': 277, 'saddest': 278, 'girl': 279, 'everyone': 280, 'yanice': 281, 'opened': 282, 'fridge': 283, 'eat': 284, 'however': 285, 'there': 286, 'leftovers': 287, 'mixed': 288, 'an': 289, 'attempt': 290, 'lunch': 291, 'place': 292, 'meat': 293, 'also': 294, 'fried': 295, 'eggs': 296, 'ended': 297, 'enjoying': 298, 'meal': 299, 'friend': 300, 'joe': 301, 'sitting': 302, 'lobby': 303, 'kept': 304, 'company': 305, 'lonely': 306, 'old': 307, 'me': 308, 'beethoven': 309, 'ninth': 310, 'hour': 311, 'left': 312, 'see': 313, 'soon': 314, 'twas': 315, 'junior': 316, 'high': 317, 'amy': 318, 'beth': 319, 'phone': 320, 'lot': 321, 'catch': 322, '2nd': 323, 'because': 324, 'knew': 325, 'hers': 326, 'better': 327, 'young': 328, 'who': 329, 'won': 330, 'lottery': 331, 'used': 332, 'lawn': 333, 'mowers': 334, 'using': 335, 'drugs': 336, 'blew': 337, 'money': 338, 'eventually': 339, 'winnings': 340, 'revoked': 341, 'dui': 342, 'die': 343, 'shopper': 344, 'waiting': 345, 'line': 346, 'outside': 347, 'miserably': 348, 'cold': 349, 'homeless': 350, 'shivering': 351, 'alleyway': 352, 'gave': 353, 'gift': 354, 'nice': 355, 'warm': 356, 'blanket': 357, 'jeff': 358, 'invited': 359, 'play': 360, 'board': 361, 'games': 362, 'saturday': 363, 'arrived': 364, 'house': 365, 'early': 366, 'evening': 367, 'six': 368, 'them': 369, 'sat': 370, 'around': 371, 'big': 372, 'table': 373, 'took': 374, 'turns': 375, 'deciding': 376, 'which': 377, 'game': 378, 'hours': 379, 'playing': 380, 'different': 381, 'chuck': 382, 'reclined': 383, 'porch': 384, 'sipped': 385, 'coffee': 386, 'screen': 387, 'this': 388, 'gathered': 389, 'tools': 390, 'material': 391, 'supplies': 392, 'labored': 393, 'all': 394, 'finish': 395, 'job': 396, 'snuggled': 397, 'wife': 398, 'bug': 399, 'free': 400, 'jessica': 401, 'along': 402, 'great': 403, 'time': 404, 'covered': 405, 'sticky': 406, 'searched': 407, 'shower': 408, 'felt': 409, 'ages': 410, 'best': 411, 'trip': 412, 'ever': 413, 'pet': 414, 'christmas': 415, 'say': 416, 'anything': 417, 'wonderful': 418, 'surprise': 419, 'received': 420, 'puppy': 421, 'shiny': 422, 'bow': 423, 'head': 424, 'kelly': 425, 'hot': 426, 'contest': 427, 'girls': 428, 'competed': 429, 'against': 430, 'each': 431, 'other': 432, 'tasting': 433, 'medal': 434, 'watching': 435, 'youtube': 436, 'video': 437, 'kitchen': 438, 'doing': 439, 'dishes': 440, 'started': 441, 'crying': 442, 'seen': 443, 'trampled': 444, 'by': 445, 'elephant': 446, 'off': 447, 'limits': 448, 'move': 449, 'no': 450, 'pay': 451, 'bought': 452, 'scratching': 453, 'ticket': 454, 'enough': 455, 'payment': 456, 'moving': 457, 'looking': 458, 'rock': 459, 'ground': 460, 'pain': 461, 'thankfully': 462, 'stranger': 463, 'rushed': 464, 'pick': 465, 'hospital': 466, 'seek': 467, 'treatment': 468, 'lily': 469, 'drove': 470, 'town': 471, 'errands': 472, 'large': 473, 'iced': 474, 'delicious': 475, 'refreshing': 476, 'wait': 477, 'put': 478, 'roof': 479, 'fishing': 480, 'keys': 481, 'fell': 482, 'spilled': 483, 'todd': 484, 'hungry': 485, 'cook': 486, 'need': 487, 'buy': 488, 'way': 489, 'store': 490, 'hamburgers': 491, 'buys': 492, 'everything': 493, 'goes': 494, 'cooks': 495, 'virgil': 496, 'bright': 497, 'blue': 498, 'recliner': 499, 'online': 500, 'hideous': 501, 'clashed': 502, 'decor': 503, 'fabric': 504, 'reupholstered': 505, 'complained': 506, 'saying': 507, 'still': 508, 'threw': 509, 'jenna': 510, 'community': 511, 'pool': 512, 'family': 513, 'deeper': 514, 'end': 515, 'herself': 516, 'without': 517, 'telling': 518, 'anyone': 519, 'farther': 520, 'lost': 521, 'footing': 522, 'lifeguard': 523, 'help': 524, 'water': 525, 'lesson': 526, 'joan': 527, 'entered': 528, 'confessional': 529, 'kneeled': 530, 'confessing': 531, 'parish': 532, 'priest': 533, 'confessed': 534, 'fantasized': 535, 'visiting': 536, 'relief': 537, 'then': 538, 'pull': 539, 'homer': 540, 'watch': 541, 'movie': 542, 'theater': 543, 'sit': 544, 'spot': 545, 'bunch': 546, 'kids': 547, 'lots': 548, 'noise': 549, 'became': 550, 'annoyed': 551, 'aisle': 552, 'chip': 553, 'loved': 554, 'dip': 555, 'party': 556, 'double': 557, 'dipped': 558, 'skip': 559, 'flip': 560, 'seeing': 561, 'punch': 562, 'lip': 563, 'anna': 564, 'invite': 565, 'peter': 566, 'sadie': 567, 'hawkins': 568, 'dance': 569, 'very': 570, 'cute': 571, 'popular': 572, 'feared': 573, 'far': 574, 'league': 575, 'summoned': 576, 'courage': 577, 'expecting': 578, 'rejection': 579, 'joy': 580, 'happily': 581, 'agreed': 582, 'be': 583, 'gina': 584, 'being': 585, 'mean': 586, 'boy': 587, 'class': 588, 'bully': 589, 'picking': 590, 'should': 591, 'stop': 592, 'apologize': 593, 'jerry': 594, 'making': 595, 'toast': 596, 'set': 597, 'medium': 598, 'came': 599, 'completely': 600, 'burnt': 601, 'tried': 602, 'settings': 603, 'results': 604, 'toaster': 605, 'last': 606, 'week': 607, 'accidentally': 608, 'overdrafted': 609, 'account': 610, 'restaurant': 611, 'charged': 612, 'too': 613, 'much': 614, 'mistake': 615, 'afterward': 616, 'five': 617, 'purchases': 618, 'hundreds': 619, 'dollars': 620, 'overdraft': 621, 'fees': 622, 'bank': 623, 'refused': 624, 'reverse': 625, 'than': 626, 'half': 627, 'laughing': 628, 'trash': 629, 'get': 630, 'posted': 631, '10000': 632, 'views': 633, 'famous': 634, 'ty': 635, 'deaf': 636, 'life': 637, 'hoping': 638, 'hear': 639, 'doctor': 640, 'offered': 641, 'kind': 642, 'super': 643, '-': 644, 'powered': 645, 'hearing': 646, 'aid': 647, 'implanted': 648, 'waited': 649, 'eagerly': 650, 'sound': 651, 'heard': 652, 'voices': 653, 'music': 654, 'those': 655, 'sounds': 656, 'musician': 657, 'bob': 658, 'ship': 659, 'elevator': 660, 'voice': 661, 'speaker': 662, 'sounded': 663, 'audio': 664, 'book': 665, 'surprised': 666, 'hallway': 667, 'cabin': 668, 'called': 669, 'steward': 670, 'coming': 671, 'pocket': 672, 'john': 673, 'sleepy': 674, 'starts': 675, 'pot': 676, 'puts': 677, 'cream': 678, 'sugar': 679, 'cup': 680, 'thermos': 681, 'adds': 682, 'both': 683, 'finishing': 684, 'takes': 685, 'work': 686, 'susan': 687, 'excited': 688, 'plan': 689, 'egg': 690, 'hunt': 691, 'south': 692, 'fun': 693, 'hide': 694, 'candy': 695, 'filled': 696, 'sun': 697, 'sky': 698, 'opening': 699, 'bursting': 700, 'tears': 701, 'inside': 702, 'melted': 703, 'met': 704, 'ann': 705, 'dating': 706, 'lutheran': 707, 'catholic': 708, 'disapproved': 709, 'religion': 710, 'wants': 711, 'marry': 712, 'someday': 713, 'worried': 714, 'ca': 715, 'jen': 716, 'laws': 717, 'frustrated': 718, 'point': 719, 'ask': 720, 'weight': 721, 'gained': 722, 'few': 723, 'pounds': 724, 'two': 725, 'weeks': 726, 'looked': 727, 'thin': 728, 'cake': 729, 'woodworker': 730, 'satisfied': 731, 'cuts': 732, 'machine': 733, 'worn': 734, 'use': 735, 'sharpener': 736, 'grind': 737, 'good': 738, 'locker': 739, 'pressed': 740, 'foot': 741, 'broken': 742, 'tile': 743, 'willing': 744, 'teachers': 745, 'area': 746, 'helped': 747, 'nurse': 748, 'timmy': 749, 'always': 750, 'obsessed': 751, 'airplanes': 752, 'dream': 753, 'pilot': 754, '16th': 755, 'birthday': 756, 'flying': 757, 'lessons': 758, 'every': 759, 'minute': 760, 'sure': 761, 'do': 762, 'spa': 763, 'relaxing': 764, 'massages': 765, 'facials': 766, 'soaked': 767, 'stayed': 768, 'sauna': 769, 'greg': 770, 'join': 771, 'marching': 772, 'band': 773, 'practiced': 774, 'past': 775, 'auditions': 776, 'even': 777, 'instrument': 778, 'tryouts': 779, 'accepted': 780, 'lisa': 781, 'has': 782, 'beautiful': 783, 'sapphire': 784, 'ring': 785, 'wash': 786, 'hands': 787, 'afternoon': 788, 'noticed': 789, 'missing': 790, 'finger': 791, 'everywhere': 792, 'elated': 793, 'bathroom': 794, 'floor': 795, 'usually': 796, 'kim': 797, 'lived': 798, 'drink': 799, 'oatmeal': 800, 'glad': 801, 'weather': 802, 'next': 803, 'enjoy': 804, 'drinks': 805, 'bus': 806, 'driver': 807, 'save': 808, 'gas': 809, 'come': 810, 'full': 811, 'slowed': 812, 'people': 813, 'hop': 814, 'jumped': 815, 'doorway': 816, 'missed': 817, 'you': 818, 'deserve': 819, 'gary': 820, 'through': 821, 'snacks': 822, 'small': 823, 'bite': 824, 'marks': 825, 'determined': 826, 'mice': 827, 'local': 828, 'exterminator': 829, 'quickly': 830, 'killed': 831, 'rats': 832, 'peace': 833, 'natalie': 834, 'auditioned': 835, 'lead': 836, 'part': 837, 'rehearsed': 838, 'acted': 839, 'little': 840, 'heart': 841, 'huge': 842, 'success': 843, 'mr': 844, 'presenting': 845, 'volcanic': 846, 'eruption': 847, 'science': 848, 'diagram': 849, 'volcano': 850, 'tinfoil': 851, 'thing': 852, 'vinegar': 853, 'pour': 854, 'clue': 855, 'astonishment': 856, 'exploded': 857, 'substance': 858, 'lava': 859, 'samantha': 860, 'taught': 861, 'self': 862, 'sufficient': 863, 'tire': 864, 'driving': 865, 'able': 866, 'properly': 867, 'grateful': 868, 'safely': 869, 'pig': 870, 'wandered': 871, 'onto': 872, 'farm': 873, 'keep': 874, 'secret': 875, 'shed': 876, 'edge': 877, 'played': 878, 'everyday': 879, 'gone': 880, 'hope': 881, 'passed': 882, 'park': 883, 'zoo': 884, 'remembered': 885, 'child': 886, 'turned': 887, 'feeling': 888, 'nostalgic': 889, 'deer': 890, 'ones': 891, 'once': 892, 'fed': 893, 'hand': 894, 'these': 895, 'scrawny': 896, 'mangy': 897, 'eyes': 898, 'wished': 899, 'never': 900, 'stopped': 901, 'mary': 902, 'ready': 903, 'relax': 904, 'popcorn': 905, 'anticipated': 906, 'unwind': 907, 'jill': 908, 'ski': 909, 'bunny': 910, 'slope': 911, 'sad': 912, 'why': 913, 'bunnies': 914, 'mark': 915, 'likes': 916, 'guitar': 917, 'booked': 918, 'gig': 919, 'coffeeshop': 920, '2': 921, '50': 922, 'showed': 923, 'applauded': 924, 'packed': 925, 'equipment': 926, 'billy': 927, 'highway': 928, 'under': 929, 'hood': 930, 'starter': 931, 'nearest': 932, 'mechanic': 933, 'quoted': 934, '300': 935, 'instead': 936, 'fixed': 937, '$': 938, '100': 939, 'functioning': 940, 'engine': 941, 'frankie': 942, 'shopping': 943, 'gifts': 944, 'cart': 945, 'paid': 946, 'things': 947, 'rex': 948, 'given': 949, 'any': 950, 'dreams': 951, 'becoming': 952, 'father': 953, 'woman': 954, 'liked': 955, 'despite': 956, 'shortcomings': 957, 'married': 958, 'son': 959, 'proud': 960, 'laura': 961, 'graduated': 962, 'college': 963, 'planning': 964, 'california': 965, 'belongings': 966, '18': 967, 'apartment': 968, 'unpacked': 969, 'scenery': 970, 'mia': 971, 'living': 972, 'room': 973, 'sports': 974, 'favorite': 975, 'soccer': 976, 'team': 977, 'rival': 978, 'encourage': 979, ' ': 980, 'chanting': 981, 'positive': 982, 'phrases': 983, 'chant': 984, 'scored': 985, 'goal': 986, 'cheered': 987, 'loudly': 988, 'score': 989, 'shannon': 990, 'sees': 991, 'right': 992, 'escape': 993, 'hits': 994, 'cars': 995, 'are': 996, 'wrecked': 997, 'alright': 998, 'though': 999, 'nate': 1000, 'calling': 1001, 'diana': 1002, 'directions': 1003, 'run': 1004, 'cafeteria': 1005, 'whole': 1006, 'divorce': 1007, 'sandy': 1008, 'alone': 1009, 'socialize': 1010, 'demurred': 1011, 'descended': 1012, 'upon': 1013, 'movies': 1014, 'normal': 1015, 'regularly': 1016, 'bogart': 1017, 'bacon': 1018, 'shortly': 1019, 'grew': 1020, 'fond': 1021, 'eating': 1022, 'preschoolers': 1023, 'field': 1024, 'fire': 1025, 'station': 1026, 'firefighters': 1027, 'truck': 1028, 'behind': 1029, 'rude': 1030, 'obnoxious': 1031, 'dread': 1032, 'type': 1033, 'olivia': 1034, 'ballerina': 1035, 'internationally': 1036, 'opportunity': 1037, 'audition': 1038, 'amazingly': 1039, 'tim': 1040, 'salesman': 1041, 'worked': 1042, 'electronic': 1043, 'customers': 1044, 'unsure': 1045, 'convinced': 1046, 'extended': 1047, 'warranty': 1048, 'april': 1049, 'fools': 1050, 'pranks': 1051, 'sneaking': 1052, 'shouted': 1053, 'chair': 1054, 'breaking': 1055, 'laugh': 1056, 'betty': 1057, 'craving': 1058, 'mint': 1059, 'ice': 1060, 'parlor': 1061, 'grocery': 1062, 'either': 1063, 'buying': 1064, 'cade': 1065, 'short': 1066, 'gets': 1067, 'picked': 1068, 'mother': 1069, 'tells': 1070, 'will': 1071, 'grow': 1072, 'tall': 1073, 'within': 1074, 'months': 1075, 'inch': 1076, 'year': 1077, 'tallest': 1078, 'longer': 1079, 'amber': 1080, 'really': 1081, 'dark': 1082, 'road': 1083, 'flood': 1084, 'messed': 1085, 'towed': 1086, 'erica': 1087, 'thanksgiving': 1088, 'chicken': 1089, 'pie': 1090, 'ingredients': 1091, 'oven': 1092, 'bake': 1093, 'neighbor': 1094, 'ben': 1095, 'mess': 1096, 'plants': 1097, 'knocked': 1098, 'newspaper': 1099, 'shredded': 1100, 'sternly': 1101, 'hiding': 1102, 'done': 1103, 'sarah': 1104, 'pee': 1105, 'badly': 1106, 'hold': 1107, 'james': 1108, 'hauled': 1109, 'wooden': 1110, 'logs': 1111, 'craigslist': 1112, 'red': 1113, 'seller': 1114, 'discount': 1115, 'susie': 1116, 'sells': 1117, '31': 1118, 'products': 1119, 'extra': 1120, 'totes': 1121, 'sold': 1122, 'typically': 1123, 'stuff': 1124, 'makes': 1125, 'easier': 1126, 'travel': 1127, 'stylish': 1128, 'selling': 1129, 'mike': 1130, 'dinner': 1131, 'pasta': 1132, 'sauce': 1133, 'tomatoes': 1134, 'fresh': 1135, 'vegetables': 1136, 'herbs': 1137, 'garden': 1138, 'heat': 1139, 'view': 1140, 'fighting': 1141, 'women': 1142, 'topics': 1143, 'disagreed': 1144, 'war': 1145, 'topic': 1146, 'polarized': 1147, 'rosie': 1148, 'elizabeth': 1149, 'fight': 1150, 'producers': 1151, 'splitscreen': 1152, 'elliott': 1153, 'tennis': 1154, 'number': 1155, '1': 1156, 'ranking': 1157, 'match': 1158, 'pouring': 1159, 'rain': 1160, 'coach': 1161, 'unfair': 1162, 'replied': 1163, '\"': 1164, 'raining': 1165, 'sides': 1166, 'court': 1167, '?': 1168, 'baby': 1169, 'helping': 1170, 'thinking': 1171, 'feeding': 1172, 'find': 1173, 'start': 1174, 'louisa': 1175, 'epcot': 1176, 'contain': 1177, 'excitement': 1178, 'moment': 1179, 'pictures': 1180, 'ten': 1181, 'joanie': 1182, 'signed': 1183, 'swimming': 1184, 'lake': 1185, 'rode': 1186, 'take': 1187, 'final': 1188, 'test': 1189, 'boat': 1190, 'shore': 1191, 'petrified': 1192, 'prayed': 1193, 'spared': 1194, 'jogging': 1195, 'running': 1196, 'shoes': 1197, 'rainy': 1198, 'rained': 1199, 'risk': 1200, 'woke': 1201, 'sick': 1202, 'washed': 1203, 'face': 1204, 'soup': 1205, 'bowl': 1206, 'microwave': 1207, 'dropped': 1208, 'grabbed': 1209, 'tyrese': 1210, 'joined': 1211, 'gym': 1212, 'membership': 1213, 'allows': 1214, 'distracted': 1215, 'grandfather': 1216, 'died': 1217, 'motivation': 1218, 'ryder': 1219, 'owner': 1220, 'door': 1221, 'smelling': 1222, 'dead': 1223, 'animal': 1224, 'bath': 1225, 'jimmy': 1226, 'police': 1227, 'officer': 1228, 'chicago': 1229, 'only': 1230, 'responds': 1231, 'calls': 1232, 'partner': 1233, 'nerves': 1234, 'news': 1235, 'tony': 1236, 'fill': 1237, 'tank': 1238, 'owe': 1239, 'someone': 1240, 'else': 1241, 'blessed': 1242, 'getting': 1243, 'taylor': 1244, 'named': 1245, 'impression': 1246, 'questions': 1247, 'hopes': 1248, 'finding': 1249, 'affinity': 1250, 'player': 1251, 'forming': 1252, 'together': 1253, 'moved': 1254, 'our': 1255, 'condo': 1256, '2013': 1257, '1987': 1258, 'boxes': 1259, 'trouble': 1260, 'lifting': 1261, 'managed': 1262, 'scared': 1263, 'encouraged': 1264, 'trevor': 1265, 'spend': 1266, 'doorbell': 1267, 'couch': 1268, 'haircut': 1269, 'stylist': 1270, 'cut': 1271, 'charge': 1272, 'fix': 1273, 'hair': 1274, '<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "print(make_lexicon(token_seqs=train_stories['Tokenized_Story'], min_freq=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's check if you just. You should get this result by executing the code below.\n",
    "````\n",
    "{'hello': 2, ',': 3, 'my': 4, 'name': 5, 'is': 6, 'ludo': 7, '.': 8, 'i': 9, 'learn': 10, 'deep': 11, 'learning': 12, 'and': 13, 'like': 14, 'it': 15, '!': 16, 'this': 17, 'the': 18, 'second': 19, 'story': 20, 'there': 21, 'no': 22, 'beginning': 23, 'end': 24, '<UNK>': 1}\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('example_model/lexicon.pkl', 'wb') as f: # Save the lexicon by pickling it\n",
    "    pickle.dump(lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's okay then you can save your in the lexicon variable.   \n",
    "\n",
    "**Exercise :** Create a lexicon variable that will contain the lexicon of ``train_stories['Tokenized_Story'] ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON SAMPLE (1274 total items):\n",
      "{'dan': 2, \"'s\": 3, 'parents': 4, 'were': 5, 'overweight': 6, '.': 7, 'was': 8, 'as': 9, 'well': 10, 'the': 11, 'doctors': 12, 'told': 13, 'his': 14, 'it': 15, 'unhealthy': 16, 'understood': 17, 'and': 18, 'decided': 19, 'to': 20, 'make': 21}\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE (1 line)\n",
    "lexicon = make_lexicon(token_seqs=train_stories['Tokenized_Story'], min_freq=1)\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the opportunity to save our lexicon for later use. And you already know the library that allows you to do that. It's about pickle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('example_model/lexicon.pkl', 'wb') as f: # Save the lexicon by pickling it\n",
    "    pickle.dump(lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the model to generation later, it will output words as indices, so we'll need to map each numerical index back to its corresponding string representation. We'll reverse the lexicon dictionary so that a word can be looked up by its index.To do this, we will create a get_lexicon_lookup() function with a parameter that will be the lexicon.\n",
    "\n",
    "**Exercise :**  Create a function that will return a dictionary where the string representation of a lexicon item can be retrieved from its numerical index. You must specify in your dictionary that the value 0 is equal to  ``\"\"``.\n",
    "\n",
    "Example : ``{2: 'hello', 3: ',', 4: 'my', 5: 'name', 0 :\"\"}``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'dan', 3: \"'s\", 4: 'parents', 5: 'were', 6: 'overweight', 7: '.', 8: 'was', 9: 'as', 10: 'well', 11: 'the', 12: 'doctors', 13: 'told', 14: 'his', 15: 'it', 16: 'unhealthy', 17: 'understood', 18: 'and', 19: 'decided', 20: 'to', 21: 'make'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 'dan',\n",
       " 3: \"'s\",\n",
       " 4: 'parents',\n",
       " 5: 'were',\n",
       " 6: 'overweight',\n",
       " 7: '.',\n",
       " 8: 'was',\n",
       " 9: 'as',\n",
       " 10: 'well',\n",
       " 11: 'the',\n",
       " 12: 'doctors',\n",
       " 13: 'told',\n",
       " 14: 'his',\n",
       " 15: 'it',\n",
       " 16: 'unhealthy',\n",
       " 17: 'understood',\n",
       " 18: 'and',\n",
       " 19: 'decided',\n",
       " 20: 'to',\n",
       " 21: 'make',\n",
       " 22: 'a',\n",
       " 23: 'change',\n",
       " 24: 'they',\n",
       " 25: 'got',\n",
       " 26: 'themselves',\n",
       " 27: 'on',\n",
       " 28: 'diet',\n",
       " 29: 'carrie',\n",
       " 30: 'had',\n",
       " 31: 'just',\n",
       " 32: 'learned',\n",
       " 33: 'how',\n",
       " 34: 'ride',\n",
       " 35: 'bike',\n",
       " 36: 'she',\n",
       " 37: 'did',\n",
       " 38: \"n't\",\n",
       " 39: 'have',\n",
       " 40: 'of',\n",
       " 41: 'her',\n",
       " 42: 'own',\n",
       " 43: 'would',\n",
       " 44: 'sneak',\n",
       " 45: 'rides',\n",
       " 46: 'sister',\n",
       " 47: 'nervous',\n",
       " 48: 'hill',\n",
       " 49: 'crashed',\n",
       " 50: 'into',\n",
       " 51: 'wall',\n",
       " 52: 'frame',\n",
       " 53: 'bent',\n",
       " 54: 'deep',\n",
       " 55: 'gash',\n",
       " 56: 'leg',\n",
       " 57: 'morgan',\n",
       " 58: 'enjoyed',\n",
       " 59: 'long',\n",
       " 60: 'walks',\n",
       " 61: 'beach',\n",
       " 62: 'boyfriend',\n",
       " 63: 'go',\n",
       " 64: 'for',\n",
       " 65: 'walk',\n",
       " 66: 'after',\n",
       " 67: 'walking',\n",
       " 68: 'over',\n",
       " 69: 'mile',\n",
       " 70: ',',\n",
       " 71: 'something',\n",
       " 72: 'happened',\n",
       " 73: 'propose',\n",
       " 74: 'upset',\n",
       " 75: 'he',\n",
       " 76: 'first',\n",
       " 77: 'jane',\n",
       " 78: 'working',\n",
       " 79: 'at',\n",
       " 80: 'diner',\n",
       " 81: 'suddenly',\n",
       " 82: 'customer',\n",
       " 83: 'barged',\n",
       " 84: 'up',\n",
       " 85: 'counter',\n",
       " 86: 'began',\n",
       " 87: 'yelling',\n",
       " 88: 'about',\n",
       " 89: 'food',\n",
       " 90: 'taking',\n",
       " 91: 'know',\n",
       " 92: 'react',\n",
       " 93: 'luckily',\n",
       " 94: 'coworker',\n",
       " 95: 'intervened',\n",
       " 96: 'calmed',\n",
       " 97: 'man',\n",
       " 98: 'down',\n",
       " 99: 'i',\n",
       " 100: 'talking',\n",
       " 101: 'my',\n",
       " 102: 'crush',\n",
       " 103: 'today',\n",
       " 104: 'continued',\n",
       " 105: 'complain',\n",
       " 106: 'guys',\n",
       " 107: 'flirting',\n",
       " 108: 'with',\n",
       " 109: 'agree',\n",
       " 110: 'what',\n",
       " 111: 'says',\n",
       " 112: 'listened',\n",
       " 113: 'patiently',\n",
       " 114: 'home',\n",
       " 115: 'text',\n",
       " 116: 'from',\n",
       " 117: 'asked',\n",
       " 118: 'if',\n",
       " 119: 'we',\n",
       " 120: 'can',\n",
       " 121: 'hang',\n",
       " 122: 'out',\n",
       " 123: 'tomorrow',\n",
       " 124: 'frank',\n",
       " 125: 'been',\n",
       " 126: 'drinking',\n",
       " 127: 'beer',\n",
       " 128: 'call',\n",
       " 129: 'girlfriend',\n",
       " 130: 'asking',\n",
       " 131: 'where',\n",
       " 132: 'realized',\n",
       " 133: 'date',\n",
       " 134: 'that',\n",
       " 135: 'night',\n",
       " 136: 'since',\n",
       " 137: 'already',\n",
       " 138: 'bit',\n",
       " 139: 'drunk',\n",
       " 140: 'could',\n",
       " 141: 'not',\n",
       " 142: 'drive',\n",
       " 143: 'spent',\n",
       " 144: 'rest',\n",
       " 145: 'more',\n",
       " 146: 'beers',\n",
       " 147: 'dave',\n",
       " 148: 'in',\n",
       " 149: 'bahamas',\n",
       " 150: 'vacation',\n",
       " 151: 'snorkeling',\n",
       " 152: 'second',\n",
       " 153: 'day',\n",
       " 154: 'while',\n",
       " 155: 'saw',\n",
       " 156: 'cave',\n",
       " 157: 'ahead',\n",
       " 158: 'went',\n",
       " 159: 'terrified',\n",
       " 160: 'when',\n",
       " 161: 'found',\n",
       " 162: 'shark',\n",
       " 163: '!',\n",
       " 164: 'swam',\n",
       " 165: 'away',\n",
       " 166: 'fast',\n",
       " 167: 'but',\n",
       " 168: 'caught',\n",
       " 169: 'ate',\n",
       " 170: 'sunny',\n",
       " 171: 'going',\n",
       " 172: 'stepped',\n",
       " 173: 'car',\n",
       " 174: 'forgot',\n",
       " 175: 'quite',\n",
       " 176: 'sunglasses',\n",
       " 177: 'back',\n",
       " 178: 'heading',\n",
       " 179: 'towards',\n",
       " 180: 'mall',\n",
       " 181: 'some',\n",
       " 182: 'headed',\n",
       " 183: 'sally',\n",
       " 184: 'happy',\n",
       " 185: 'widowed',\n",
       " 186: 'mom',\n",
       " 187: 'new',\n",
       " 188: 'discovered',\n",
       " 189: 'siblings',\n",
       " 190: 'feel',\n",
       " 191: 'same',\n",
       " 192: 'flew',\n",
       " 193: 'visit',\n",
       " 194: 'husband',\n",
       " 195: 'although',\n",
       " 196: 'obviously',\n",
       " 197: 'love',\n",
       " 198: 'nothing',\n",
       " 199: 'like',\n",
       " 200: 'dad',\n",
       " 201: 'wondered',\n",
       " 202: \"'\",\n",
       " 203: 'marriage',\n",
       " 204: 'hit',\n",
       " 205: 'golf',\n",
       " 206: 'ball',\n",
       " 207: 'watched',\n",
       " 208: 'bounced',\n",
       " 209: 'grass',\n",
       " 210: 'sand',\n",
       " 211: 'trap',\n",
       " 212: 'pretended',\n",
       " 213: 'actually',\n",
       " 214: 'landed',\n",
       " 215: 'green',\n",
       " 216: 'friends',\n",
       " 217: 'paying',\n",
       " 218: 'attention',\n",
       " 219: 'so',\n",
       " 220: 'believed',\n",
       " 221: 'him',\n",
       " 222: 'snuck',\n",
       " 223: 'made',\n",
       " 224: 'putt',\n",
       " 225: '10',\n",
       " 226: 'feet',\n",
       " 227: 'josh',\n",
       " 228: 'parrot',\n",
       " 229: 'talked',\n",
       " 230: 'brought',\n",
       " 231: 'school',\n",
       " 232: 'during',\n",
       " 233: 'show',\n",
       " 234: 'tell',\n",
       " 235: 'said',\n",
       " 236: 'bad',\n",
       " 237: 'word',\n",
       " 238: 'teacher',\n",
       " 239: 'joshua',\n",
       " 240: 'bring',\n",
       " 241: 'bird',\n",
       " 242: 'again',\n",
       " 243: 'grounded',\n",
       " 244: 'hal',\n",
       " 245: 'dog',\n",
       " 246: 'one',\n",
       " 247: 'morning',\n",
       " 248: 'cat',\n",
       " 249: 'ran',\n",
       " 250: 'across',\n",
       " 251: 'their',\n",
       " 252: 'path',\n",
       " 253: 'strained',\n",
       " 254: 'hard',\n",
       " 255: 'leash',\n",
       " 256: 'broke',\n",
       " 257: 'chased',\n",
       " 258: 'several',\n",
       " 259: 'minutes',\n",
       " 260: 'finally',\n",
       " 261: 'lured',\n",
       " 262: 'side',\n",
       " 263: 'brenda',\n",
       " 264: 'maxwell',\n",
       " 265: 'successful',\n",
       " 266: 'artist',\n",
       " 267: 'promising',\n",
       " 268: 'future',\n",
       " 269: 'needed',\n",
       " 270: 'talk',\n",
       " 271: 'thought',\n",
       " 272: \"'d\",\n",
       " 273: 'wanted',\n",
       " 274: 'break',\n",
       " 275: 'walked',\n",
       " 276: 'now',\n",
       " 277: 'is',\n",
       " 278: 'saddest',\n",
       " 279: 'girl',\n",
       " 280: 'everyone',\n",
       " 281: 'yanice',\n",
       " 282: 'opened',\n",
       " 283: 'fridge',\n",
       " 284: 'eat',\n",
       " 285: 'however',\n",
       " 286: 'there',\n",
       " 287: 'leftovers',\n",
       " 288: 'mixed',\n",
       " 289: 'an',\n",
       " 290: 'attempt',\n",
       " 291: 'lunch',\n",
       " 292: 'place',\n",
       " 293: 'meat',\n",
       " 294: 'also',\n",
       " 295: 'fried',\n",
       " 296: 'eggs',\n",
       " 297: 'ended',\n",
       " 298: 'enjoying',\n",
       " 299: 'meal',\n",
       " 300: 'friend',\n",
       " 301: 'joe',\n",
       " 302: 'sitting',\n",
       " 303: 'lobby',\n",
       " 304: 'kept',\n",
       " 305: 'company',\n",
       " 306: 'lonely',\n",
       " 307: 'old',\n",
       " 308: 'me',\n",
       " 309: 'beethoven',\n",
       " 310: 'ninth',\n",
       " 311: 'hour',\n",
       " 312: 'left',\n",
       " 313: 'see',\n",
       " 314: 'soon',\n",
       " 315: 'twas',\n",
       " 316: 'junior',\n",
       " 317: 'high',\n",
       " 318: 'amy',\n",
       " 319: 'beth',\n",
       " 320: 'phone',\n",
       " 321: 'lot',\n",
       " 322: 'catch',\n",
       " 323: '2nd',\n",
       " 324: 'because',\n",
       " 325: 'knew',\n",
       " 326: 'hers',\n",
       " 327: 'better',\n",
       " 328: 'young',\n",
       " 329: 'who',\n",
       " 330: 'won',\n",
       " 331: 'lottery',\n",
       " 332: 'used',\n",
       " 333: 'lawn',\n",
       " 334: 'mowers',\n",
       " 335: 'using',\n",
       " 336: 'drugs',\n",
       " 337: 'blew',\n",
       " 338: 'money',\n",
       " 339: 'eventually',\n",
       " 340: 'winnings',\n",
       " 341: 'revoked',\n",
       " 342: 'dui',\n",
       " 343: 'die',\n",
       " 344: 'shopper',\n",
       " 345: 'waiting',\n",
       " 346: 'line',\n",
       " 347: 'outside',\n",
       " 348: 'miserably',\n",
       " 349: 'cold',\n",
       " 350: 'homeless',\n",
       " 351: 'shivering',\n",
       " 352: 'alleyway',\n",
       " 353: 'gave',\n",
       " 354: 'gift',\n",
       " 355: 'nice',\n",
       " 356: 'warm',\n",
       " 357: 'blanket',\n",
       " 358: 'jeff',\n",
       " 359: 'invited',\n",
       " 360: 'play',\n",
       " 361: 'board',\n",
       " 362: 'games',\n",
       " 363: 'saturday',\n",
       " 364: 'arrived',\n",
       " 365: 'house',\n",
       " 366: 'early',\n",
       " 367: 'evening',\n",
       " 368: 'six',\n",
       " 369: 'them',\n",
       " 370: 'sat',\n",
       " 371: 'around',\n",
       " 372: 'big',\n",
       " 373: 'table',\n",
       " 374: 'took',\n",
       " 375: 'turns',\n",
       " 376: 'deciding',\n",
       " 377: 'which',\n",
       " 378: 'game',\n",
       " 379: 'hours',\n",
       " 380: 'playing',\n",
       " 381: 'different',\n",
       " 382: 'chuck',\n",
       " 383: 'reclined',\n",
       " 384: 'porch',\n",
       " 385: 'sipped',\n",
       " 386: 'coffee',\n",
       " 387: 'screen',\n",
       " 388: 'this',\n",
       " 389: 'gathered',\n",
       " 390: 'tools',\n",
       " 391: 'material',\n",
       " 392: 'supplies',\n",
       " 393: 'labored',\n",
       " 394: 'all',\n",
       " 395: 'finish',\n",
       " 396: 'job',\n",
       " 397: 'snuggled',\n",
       " 398: 'wife',\n",
       " 399: 'bug',\n",
       " 400: 'free',\n",
       " 401: 'jessica',\n",
       " 402: 'along',\n",
       " 403: 'great',\n",
       " 404: 'time',\n",
       " 405: 'covered',\n",
       " 406: 'sticky',\n",
       " 407: 'searched',\n",
       " 408: 'shower',\n",
       " 409: 'felt',\n",
       " 410: 'ages',\n",
       " 411: 'best',\n",
       " 412: 'trip',\n",
       " 413: 'ever',\n",
       " 414: 'pet',\n",
       " 415: 'christmas',\n",
       " 416: 'say',\n",
       " 417: 'anything',\n",
       " 418: 'wonderful',\n",
       " 419: 'surprise',\n",
       " 420: 'received',\n",
       " 421: 'puppy',\n",
       " 422: 'shiny',\n",
       " 423: 'bow',\n",
       " 424: 'head',\n",
       " 425: 'kelly',\n",
       " 426: 'hot',\n",
       " 427: 'contest',\n",
       " 428: 'girls',\n",
       " 429: 'competed',\n",
       " 430: 'against',\n",
       " 431: 'each',\n",
       " 432: 'other',\n",
       " 433: 'tasting',\n",
       " 434: 'medal',\n",
       " 435: 'watching',\n",
       " 436: 'youtube',\n",
       " 437: 'video',\n",
       " 438: 'kitchen',\n",
       " 439: 'doing',\n",
       " 440: 'dishes',\n",
       " 441: 'started',\n",
       " 442: 'crying',\n",
       " 443: 'seen',\n",
       " 444: 'trampled',\n",
       " 445: 'by',\n",
       " 446: 'elephant',\n",
       " 447: 'off',\n",
       " 448: 'limits',\n",
       " 449: 'move',\n",
       " 450: 'no',\n",
       " 451: 'pay',\n",
       " 452: 'bought',\n",
       " 453: 'scratching',\n",
       " 454: 'ticket',\n",
       " 455: 'enough',\n",
       " 456: 'payment',\n",
       " 457: 'moving',\n",
       " 458: 'looking',\n",
       " 459: 'rock',\n",
       " 460: 'ground',\n",
       " 461: 'pain',\n",
       " 462: 'thankfully',\n",
       " 463: 'stranger',\n",
       " 464: 'rushed',\n",
       " 465: 'pick',\n",
       " 466: 'hospital',\n",
       " 467: 'seek',\n",
       " 468: 'treatment',\n",
       " 469: 'lily',\n",
       " 470: 'drove',\n",
       " 471: 'town',\n",
       " 472: 'errands',\n",
       " 473: 'large',\n",
       " 474: 'iced',\n",
       " 475: 'delicious',\n",
       " 476: 'refreshing',\n",
       " 477: 'wait',\n",
       " 478: 'put',\n",
       " 479: 'roof',\n",
       " 480: 'fishing',\n",
       " 481: 'keys',\n",
       " 482: 'fell',\n",
       " 483: 'spilled',\n",
       " 484: 'todd',\n",
       " 485: 'hungry',\n",
       " 486: 'cook',\n",
       " 487: 'need',\n",
       " 488: 'buy',\n",
       " 489: 'way',\n",
       " 490: 'store',\n",
       " 491: 'hamburgers',\n",
       " 492: 'buys',\n",
       " 493: 'everything',\n",
       " 494: 'goes',\n",
       " 495: 'cooks',\n",
       " 496: 'virgil',\n",
       " 497: 'bright',\n",
       " 498: 'blue',\n",
       " 499: 'recliner',\n",
       " 500: 'online',\n",
       " 501: 'hideous',\n",
       " 502: 'clashed',\n",
       " 503: 'decor',\n",
       " 504: 'fabric',\n",
       " 505: 'reupholstered',\n",
       " 506: 'complained',\n",
       " 507: 'saying',\n",
       " 508: 'still',\n",
       " 509: 'threw',\n",
       " 510: 'jenna',\n",
       " 511: 'community',\n",
       " 512: 'pool',\n",
       " 513: 'family',\n",
       " 514: 'deeper',\n",
       " 515: 'end',\n",
       " 516: 'herself',\n",
       " 517: 'without',\n",
       " 518: 'telling',\n",
       " 519: 'anyone',\n",
       " 520: 'farther',\n",
       " 521: 'lost',\n",
       " 522: 'footing',\n",
       " 523: 'lifeguard',\n",
       " 524: 'help',\n",
       " 525: 'water',\n",
       " 526: 'lesson',\n",
       " 527: 'joan',\n",
       " 528: 'entered',\n",
       " 529: 'confessional',\n",
       " 530: 'kneeled',\n",
       " 531: 'confessing',\n",
       " 532: 'parish',\n",
       " 533: 'priest',\n",
       " 534: 'confessed',\n",
       " 535: 'fantasized',\n",
       " 536: 'visiting',\n",
       " 537: 'relief',\n",
       " 538: 'then',\n",
       " 539: 'pull',\n",
       " 540: 'homer',\n",
       " 541: 'watch',\n",
       " 542: 'movie',\n",
       " 543: 'theater',\n",
       " 544: 'sit',\n",
       " 545: 'spot',\n",
       " 546: 'bunch',\n",
       " 547: 'kids',\n",
       " 548: 'lots',\n",
       " 549: 'noise',\n",
       " 550: 'became',\n",
       " 551: 'annoyed',\n",
       " 552: 'aisle',\n",
       " 553: 'chip',\n",
       " 554: 'loved',\n",
       " 555: 'dip',\n",
       " 556: 'party',\n",
       " 557: 'double',\n",
       " 558: 'dipped',\n",
       " 559: 'skip',\n",
       " 560: 'flip',\n",
       " 561: 'seeing',\n",
       " 562: 'punch',\n",
       " 563: 'lip',\n",
       " 564: 'anna',\n",
       " 565: 'invite',\n",
       " 566: 'peter',\n",
       " 567: 'sadie',\n",
       " 568: 'hawkins',\n",
       " 569: 'dance',\n",
       " 570: 'very',\n",
       " 571: 'cute',\n",
       " 572: 'popular',\n",
       " 573: 'feared',\n",
       " 574: 'far',\n",
       " 575: 'league',\n",
       " 576: 'summoned',\n",
       " 577: 'courage',\n",
       " 578: 'expecting',\n",
       " 579: 'rejection',\n",
       " 580: 'joy',\n",
       " 581: 'happily',\n",
       " 582: 'agreed',\n",
       " 583: 'be',\n",
       " 584: 'gina',\n",
       " 585: 'being',\n",
       " 586: 'mean',\n",
       " 587: 'boy',\n",
       " 588: 'class',\n",
       " 589: 'bully',\n",
       " 590: 'picking',\n",
       " 591: 'should',\n",
       " 592: 'stop',\n",
       " 593: 'apologize',\n",
       " 594: 'jerry',\n",
       " 595: 'making',\n",
       " 596: 'toast',\n",
       " 597: 'set',\n",
       " 598: 'medium',\n",
       " 599: 'came',\n",
       " 600: 'completely',\n",
       " 601: 'burnt',\n",
       " 602: 'tried',\n",
       " 603: 'settings',\n",
       " 604: 'results',\n",
       " 605: 'toaster',\n",
       " 606: 'last',\n",
       " 607: 'week',\n",
       " 608: 'accidentally',\n",
       " 609: 'overdrafted',\n",
       " 610: 'account',\n",
       " 611: 'restaurant',\n",
       " 612: 'charged',\n",
       " 613: 'too',\n",
       " 614: 'much',\n",
       " 615: 'mistake',\n",
       " 616: 'afterward',\n",
       " 617: 'five',\n",
       " 618: 'purchases',\n",
       " 619: 'hundreds',\n",
       " 620: 'dollars',\n",
       " 621: 'overdraft',\n",
       " 622: 'fees',\n",
       " 623: 'bank',\n",
       " 624: 'refused',\n",
       " 625: 'reverse',\n",
       " 626: 'than',\n",
       " 627: 'half',\n",
       " 628: 'laughing',\n",
       " 629: 'trash',\n",
       " 630: 'get',\n",
       " 631: 'posted',\n",
       " 632: '10000',\n",
       " 633: 'views',\n",
       " 634: 'famous',\n",
       " 635: 'ty',\n",
       " 636: 'deaf',\n",
       " 637: 'life',\n",
       " 638: 'hoping',\n",
       " 639: 'hear',\n",
       " 640: 'doctor',\n",
       " 641: 'offered',\n",
       " 642: 'kind',\n",
       " 643: 'super',\n",
       " 644: '-',\n",
       " 645: 'powered',\n",
       " 646: 'hearing',\n",
       " 647: 'aid',\n",
       " 648: 'implanted',\n",
       " 649: 'waited',\n",
       " 650: 'eagerly',\n",
       " 651: 'sound',\n",
       " 652: 'heard',\n",
       " 653: 'voices',\n",
       " 654: 'music',\n",
       " 655: 'those',\n",
       " 656: 'sounds',\n",
       " 657: 'musician',\n",
       " 658: 'bob',\n",
       " 659: 'ship',\n",
       " 660: 'elevator',\n",
       " 661: 'voice',\n",
       " 662: 'speaker',\n",
       " 663: 'sounded',\n",
       " 664: 'audio',\n",
       " 665: 'book',\n",
       " 666: 'surprised',\n",
       " 667: 'hallway',\n",
       " 668: 'cabin',\n",
       " 669: 'called',\n",
       " 670: 'steward',\n",
       " 671: 'coming',\n",
       " 672: 'pocket',\n",
       " 673: 'john',\n",
       " 674: 'sleepy',\n",
       " 675: 'starts',\n",
       " 676: 'pot',\n",
       " 677: 'puts',\n",
       " 678: 'cream',\n",
       " 679: 'sugar',\n",
       " 680: 'cup',\n",
       " 681: 'thermos',\n",
       " 682: 'adds',\n",
       " 683: 'both',\n",
       " 684: 'finishing',\n",
       " 685: 'takes',\n",
       " 686: 'work',\n",
       " 687: 'susan',\n",
       " 688: 'excited',\n",
       " 689: 'plan',\n",
       " 690: 'egg',\n",
       " 691: 'hunt',\n",
       " 692: 'south',\n",
       " 693: 'fun',\n",
       " 694: 'hide',\n",
       " 695: 'candy',\n",
       " 696: 'filled',\n",
       " 697: 'sun',\n",
       " 698: 'sky',\n",
       " 699: 'opening',\n",
       " 700: 'bursting',\n",
       " 701: 'tears',\n",
       " 702: 'inside',\n",
       " 703: 'melted',\n",
       " 704: 'met',\n",
       " 705: 'ann',\n",
       " 706: 'dating',\n",
       " 707: 'lutheran',\n",
       " 708: 'catholic',\n",
       " 709: 'disapproved',\n",
       " 710: 'religion',\n",
       " 711: 'wants',\n",
       " 712: 'marry',\n",
       " 713: 'someday',\n",
       " 714: 'worried',\n",
       " 715: 'ca',\n",
       " 716: 'jen',\n",
       " 717: 'laws',\n",
       " 718: 'frustrated',\n",
       " 719: 'point',\n",
       " 720: 'ask',\n",
       " 721: 'weight',\n",
       " 722: 'gained',\n",
       " 723: 'few',\n",
       " 724: 'pounds',\n",
       " 725: 'two',\n",
       " 726: 'weeks',\n",
       " 727: 'looked',\n",
       " 728: 'thin',\n",
       " 729: 'cake',\n",
       " 730: 'woodworker',\n",
       " 731: 'satisfied',\n",
       " 732: 'cuts',\n",
       " 733: 'machine',\n",
       " 734: 'worn',\n",
       " 735: 'use',\n",
       " 736: 'sharpener',\n",
       " 737: 'grind',\n",
       " 738: 'good',\n",
       " 739: 'locker',\n",
       " 740: 'pressed',\n",
       " 741: 'foot',\n",
       " 742: 'broken',\n",
       " 743: 'tile',\n",
       " 744: 'willing',\n",
       " 745: 'teachers',\n",
       " 746: 'area',\n",
       " 747: 'helped',\n",
       " 748: 'nurse',\n",
       " 749: 'timmy',\n",
       " 750: 'always',\n",
       " 751: 'obsessed',\n",
       " 752: 'airplanes',\n",
       " 753: 'dream',\n",
       " 754: 'pilot',\n",
       " 755: '16th',\n",
       " 756: 'birthday',\n",
       " 757: 'flying',\n",
       " 758: 'lessons',\n",
       " 759: 'every',\n",
       " 760: 'minute',\n",
       " 761: 'sure',\n",
       " 762: 'do',\n",
       " 763: 'spa',\n",
       " 764: 'relaxing',\n",
       " 765: 'massages',\n",
       " 766: 'facials',\n",
       " 767: 'soaked',\n",
       " 768: 'stayed',\n",
       " 769: 'sauna',\n",
       " 770: 'greg',\n",
       " 771: 'join',\n",
       " 772: 'marching',\n",
       " 773: 'band',\n",
       " 774: 'practiced',\n",
       " 775: 'past',\n",
       " 776: 'auditions',\n",
       " 777: 'even',\n",
       " 778: 'instrument',\n",
       " 779: 'tryouts',\n",
       " 780: 'accepted',\n",
       " 781: 'lisa',\n",
       " 782: 'has',\n",
       " 783: 'beautiful',\n",
       " 784: 'sapphire',\n",
       " 785: 'ring',\n",
       " 786: 'wash',\n",
       " 787: 'hands',\n",
       " 788: 'afternoon',\n",
       " 789: 'noticed',\n",
       " 790: 'missing',\n",
       " 791: 'finger',\n",
       " 792: 'everywhere',\n",
       " 793: 'elated',\n",
       " 794: 'bathroom',\n",
       " 795: 'floor',\n",
       " 796: 'usually',\n",
       " 797: 'kim',\n",
       " 798: 'lived',\n",
       " 799: 'drink',\n",
       " 800: 'oatmeal',\n",
       " 801: 'glad',\n",
       " 802: 'weather',\n",
       " 803: 'next',\n",
       " 804: 'enjoy',\n",
       " 805: 'drinks',\n",
       " 806: 'bus',\n",
       " 807: 'driver',\n",
       " 808: 'save',\n",
       " 809: 'gas',\n",
       " 810: 'come',\n",
       " 811: 'full',\n",
       " 812: 'slowed',\n",
       " 813: 'people',\n",
       " 814: 'hop',\n",
       " 815: 'jumped',\n",
       " 816: 'doorway',\n",
       " 817: 'missed',\n",
       " 818: 'you',\n",
       " 819: 'deserve',\n",
       " 820: 'gary',\n",
       " 821: 'through',\n",
       " 822: 'snacks',\n",
       " 823: 'small',\n",
       " 824: 'bite',\n",
       " 825: 'marks',\n",
       " 826: 'determined',\n",
       " 827: 'mice',\n",
       " 828: 'local',\n",
       " 829: 'exterminator',\n",
       " 830: 'quickly',\n",
       " 831: 'killed',\n",
       " 832: 'rats',\n",
       " 833: 'peace',\n",
       " 834: 'natalie',\n",
       " 835: 'auditioned',\n",
       " 836: 'lead',\n",
       " 837: 'part',\n",
       " 838: 'rehearsed',\n",
       " 839: 'acted',\n",
       " 840: 'little',\n",
       " 841: 'heart',\n",
       " 842: 'huge',\n",
       " 843: 'success',\n",
       " 844: 'mr',\n",
       " 845: 'presenting',\n",
       " 846: 'volcanic',\n",
       " 847: 'eruption',\n",
       " 848: 'science',\n",
       " 849: 'diagram',\n",
       " 850: 'volcano',\n",
       " 851: 'tinfoil',\n",
       " 852: 'thing',\n",
       " 853: 'vinegar',\n",
       " 854: 'pour',\n",
       " 855: 'clue',\n",
       " 856: 'astonishment',\n",
       " 857: 'exploded',\n",
       " 858: 'substance',\n",
       " 859: 'lava',\n",
       " 860: 'samantha',\n",
       " 861: 'taught',\n",
       " 862: 'self',\n",
       " 863: 'sufficient',\n",
       " 864: 'tire',\n",
       " 865: 'driving',\n",
       " 866: 'able',\n",
       " 867: 'properly',\n",
       " 868: 'grateful',\n",
       " 869: 'safely',\n",
       " 870: 'pig',\n",
       " 871: 'wandered',\n",
       " 872: 'onto',\n",
       " 873: 'farm',\n",
       " 874: 'keep',\n",
       " 875: 'secret',\n",
       " 876: 'shed',\n",
       " 877: 'edge',\n",
       " 878: 'played',\n",
       " 879: 'everyday',\n",
       " 880: 'gone',\n",
       " 881: 'hope',\n",
       " 882: 'passed',\n",
       " 883: 'park',\n",
       " 884: 'zoo',\n",
       " 885: 'remembered',\n",
       " 886: 'child',\n",
       " 887: 'turned',\n",
       " 888: 'feeling',\n",
       " 889: 'nostalgic',\n",
       " 890: 'deer',\n",
       " 891: 'ones',\n",
       " 892: 'once',\n",
       " 893: 'fed',\n",
       " 894: 'hand',\n",
       " 895: 'these',\n",
       " 896: 'scrawny',\n",
       " 897: 'mangy',\n",
       " 898: 'eyes',\n",
       " 899: 'wished',\n",
       " 900: 'never',\n",
       " 901: 'stopped',\n",
       " 902: 'mary',\n",
       " 903: 'ready',\n",
       " 904: 'relax',\n",
       " 905: 'popcorn',\n",
       " 906: 'anticipated',\n",
       " 907: 'unwind',\n",
       " 908: 'jill',\n",
       " 909: 'ski',\n",
       " 910: 'bunny',\n",
       " 911: 'slope',\n",
       " 912: 'sad',\n",
       " 913: 'why',\n",
       " 914: 'bunnies',\n",
       " 915: 'mark',\n",
       " 916: 'likes',\n",
       " 917: 'guitar',\n",
       " 918: 'booked',\n",
       " 919: 'gig',\n",
       " 920: 'coffeeshop',\n",
       " 921: '2',\n",
       " 922: '50',\n",
       " 923: 'showed',\n",
       " 924: 'applauded',\n",
       " 925: 'packed',\n",
       " 926: 'equipment',\n",
       " 927: 'billy',\n",
       " 928: 'highway',\n",
       " 929: 'under',\n",
       " 930: 'hood',\n",
       " 931: 'starter',\n",
       " 932: 'nearest',\n",
       " 933: 'mechanic',\n",
       " 934: 'quoted',\n",
       " 935: '300',\n",
       " 936: 'instead',\n",
       " 937: 'fixed',\n",
       " 938: '$',\n",
       " 939: '100',\n",
       " 940: 'functioning',\n",
       " 941: 'engine',\n",
       " 942: 'frankie',\n",
       " 943: 'shopping',\n",
       " 944: 'gifts',\n",
       " 945: 'cart',\n",
       " 946: 'paid',\n",
       " 947: 'things',\n",
       " 948: 'rex',\n",
       " 949: 'given',\n",
       " 950: 'any',\n",
       " 951: 'dreams',\n",
       " 952: 'becoming',\n",
       " 953: 'father',\n",
       " 954: 'woman',\n",
       " 955: 'liked',\n",
       " 956: 'despite',\n",
       " 957: 'shortcomings',\n",
       " 958: 'married',\n",
       " 959: 'son',\n",
       " 960: 'proud',\n",
       " 961: 'laura',\n",
       " 962: 'graduated',\n",
       " 963: 'college',\n",
       " 964: 'planning',\n",
       " 965: 'california',\n",
       " 966: 'belongings',\n",
       " 967: '18',\n",
       " 968: 'apartment',\n",
       " 969: 'unpacked',\n",
       " 970: 'scenery',\n",
       " 971: 'mia',\n",
       " 972: 'living',\n",
       " 973: 'room',\n",
       " 974: 'sports',\n",
       " 975: 'favorite',\n",
       " 976: 'soccer',\n",
       " 977: 'team',\n",
       " 978: 'rival',\n",
       " 979: 'encourage',\n",
       " 980: ' ',\n",
       " 981: 'chanting',\n",
       " 982: 'positive',\n",
       " 983: 'phrases',\n",
       " 984: 'chant',\n",
       " 985: 'scored',\n",
       " 986: 'goal',\n",
       " 987: 'cheered',\n",
       " 988: 'loudly',\n",
       " 989: 'score',\n",
       " 990: 'shannon',\n",
       " 991: 'sees',\n",
       " 992: 'right',\n",
       " 993: 'escape',\n",
       " 994: 'hits',\n",
       " 995: 'cars',\n",
       " 996: 'are',\n",
       " 997: 'wrecked',\n",
       " 998: 'alright',\n",
       " 999: 'though',\n",
       " 1000: 'nate',\n",
       " 1001: 'calling',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    ### ENTER YOUR CODE HERE (+- 3 lines)\n",
    "\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    lexicon_lookup[0] = \"\" #map 0 padding to empty string\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup\n",
    "\n",
    "get_lexicon_lookup(lexicon)\n",
    "\n",
    "    ###END \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if your function is correct. You should have this after you execute this code. \n",
    "````\n",
    "{2: 'hello', 3: ',', 4: 'my', 5: 'name', 6: 'is', 7: 'ludo', 8: '.', 9: 'i', 10: 'learn', 11: 'deep', 12: 'learning', 13: 'and', 14: 'like', 15: 'it', 16: '!', 17: 'this', 18: 'the', 19: 'second', 20: 'story', 21: 'there'}\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON SAMPLE (24 total items):\n",
      "{'hello': 2, ',': 3, 'my': 4, 'name': 5, 'is': 6, 'ludo': 7, '.': 8, 'i': 9, 'learn': 10, 'deep': 11, 'learning': 12, 'and': 13, 'like': 14, 'it': 15, '!': 16, 'this': 17, 'the': 18, 'second': 19, 'story': 20, 'there': 21}\n",
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'hello', 3: ',', 4: 'my', 5: 'name', 6: 'is', 7: 'ludo', 8: '.', 9: 'i', 10: 'learn', 11: 'deep', 12: 'learning', 13: 'and', 14: 'like', 15: 'it', 16: '!', 17: 'this', 18: 'the', 19: 'second', 20: 'story', 21: 'there'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 'hello',\n",
       " 3: ',',\n",
       " 4: 'my',\n",
       " 5: 'name',\n",
       " 6: 'is',\n",
       " 7: 'ludo',\n",
       " 8: '.',\n",
       " 9: 'i',\n",
       " 10: 'learn',\n",
       " 11: 'deep',\n",
       " 12: 'learning',\n",
       " 13: 'and',\n",
       " 14: 'like',\n",
       " 15: 'it',\n",
       " 16: '!',\n",
       " 17: 'this',\n",
       " 18: 'the',\n",
       " 19: 'second',\n",
       " 20: 'story',\n",
       " 21: 'there',\n",
       " 22: 'no',\n",
       " 23: 'beginning',\n",
       " 24: 'end',\n",
       " 1: '<UNK>',\n",
       " 0: ''}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lexicon_lookup(make_lexicon(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** save the result on ``lexicon`` in the variable ``lexicon_lookup``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'dan', 3: \"'s\", 4: 'parents', 5: 'were', 6: 'overweight', 7: '.', 8: 'was', 9: 'as', 10: 'well', 11: 'the', 12: 'doctors', 13: 'told', 14: 'his', 15: 'it', 16: 'unhealthy', 17: 'understood', 18: 'and', 19: 'decided', 20: 'to', 21: 'make'}\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE (1 line)\n",
    "lexicon_lookup = get_lexicon_lookup(lexicon)\n",
    "\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  From strings to numbers\n",
    "\n",
    "Once the lexicon is built, we can use it to transform each story from a list of string tokens into a list of numerical indexes. This will allow us to do mathematical calculations on a string of characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Story</th>\n",
       "      <th>Story_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dan, 's, parents, were, overweight, ., dan, w...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 2, 8, 6, 9, 10, 7, 11, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[carrie, had, just, learned, how, to, ride, a,...</td>\n",
       "      <td>[29, 30, 31, 32, 33, 20, 34, 22, 35, 7, 36, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[morgan, enjoyed, long, walks, on, the, beach,...</td>\n",
       "      <td>[57, 58, 59, 60, 27, 11, 61, 7, 36, 18, 41, 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[jane, was, working, at, a, diner, ., suddenly...</td>\n",
       "      <td>[77, 8, 78, 79, 22, 80, 7, 81, 70, 22, 82, 83,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, was, talking, to, my, crush, today, ., she...</td>\n",
       "      <td>[99, 8, 100, 20, 101, 102, 103, 7, 36, 104, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[frank, had, been, drinking, beer, ., he, got,...</td>\n",
       "      <td>[124, 30, 125, 126, 127, 7, 75, 25, 22, 128, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[dave, was, in, the, bahamas, on, vacation, .,...</td>\n",
       "      <td>[147, 8, 148, 11, 149, 27, 150, 7, 75, 19, 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sunny, enjoyed, going, to, the, beach, ., as,...</td>\n",
       "      <td>[170, 58, 171, 20, 11, 61, 7, 9, 36, 172, 122,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sally, was, happy, when, her, widowed, mom, f...</td>\n",
       "      <td>[183, 8, 184, 160, 41, 185, 186, 161, 22, 187,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[dan, hit, his, golf, ball, and, watched, it, ...</td>\n",
       "      <td>[2, 204, 14, 205, 206, 18, 207, 15, 63, 7, 11,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tokenized_Story  \\\n",
       "0  [dan, 's, parents, were, overweight, ., dan, w...   \n",
       "1  [carrie, had, just, learned, how, to, ride, a,...   \n",
       "2  [morgan, enjoyed, long, walks, on, the, beach,...   \n",
       "3  [jane, was, working, at, a, diner, ., suddenly...   \n",
       "4  [i, was, talking, to, my, crush, today, ., she...   \n",
       "5  [frank, had, been, drinking, beer, ., he, got,...   \n",
       "6  [dave, was, in, the, bahamas, on, vacation, .,...   \n",
       "7  [sunny, enjoyed, going, to, the, beach, ., as,...   \n",
       "8  [sally, was, happy, when, her, widowed, mom, f...   \n",
       "9  [dan, hit, his, golf, ball, and, watched, it, ...   \n",
       "\n",
       "                                          Story_Idxs  \n",
       "0  [2, 3, 4, 5, 6, 7, 2, 8, 6, 9, 10, 7, 11, 12, ...  \n",
       "1  [29, 30, 31, 32, 33, 20, 34, 22, 35, 7, 36, 37...  \n",
       "2  [57, 58, 59, 60, 27, 11, 61, 7, 36, 18, 41, 62...  \n",
       "3  [77, 8, 78, 79, 22, 80, 7, 81, 70, 22, 82, 83,...  \n",
       "4  [99, 8, 100, 20, 101, 102, 103, 7, 36, 104, 20...  \n",
       "5  [124, 30, 125, 126, 127, 7, 75, 25, 22, 128, 1...  \n",
       "6  [147, 8, 148, 11, 149, 27, 150, 7, 75, 19, 20,...  \n",
       "7  [170, 58, 171, 20, 11, 61, 7, 9, 36, 172, 122,...  \n",
       "8  [183, 8, 184, 160, 41, 185, 186, 161, 22, 187,...  \n",
       "9  [2, 204, 14, 205, 206, 18, 207, 15, 63, 7, 11,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert each text from a list of tokens to a list of numbers (indices)'''\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=train_stories['Tokenized_Story'],\n",
    "                                             lexicon=lexicon)\n",
    "                                   \n",
    "train_stories[['Tokenized_Story', 'Story_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a matrix\n",
    "\n",
    "Finally, we need to put all the training stories into a single matrix, where each row is a story and each column is a word index in that story. This enables the model to process the stories in batches as opposed to one at a time, which significantly speeds up training. However, each story has a different number of words. So we create a padded matrix equal to the length on the longest story in the training set. For all stories with fewer words, we prepend the row with zeros, each representing an empty word position. This is why the number 0 was not assigned as a word index in the lexicon. Then we can actually tell Keras to ignore these zeros during training.\n",
    "\n",
    "**Exercise :** Create a max_seq_len variable that will contain the value of the word number  in the largest string. Get length of longest sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE \n",
    "# Get length of longest sequence \n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_stories['Story_Idxs']]) # Get length of longest sequence\n",
    "print(max_seq_len)\n",
    "\n",
    "###END "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its value should be 73 if you use the same dataset !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the sequences, pad_sequence of keras will allow us to do that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   22   28    7]\n",
      " [   0    0    0 ...   41   56    7]\n",
      " [   0    0    0 ...   41   76    7]\n",
      " ...\n",
      " [   0    0    0 ...  108   41    7]\n",
      " [   0    0    0 ...   11 1268    7]\n",
      " [   0    0    0 ...  177  122    7]]\n",
      "SHAPE: (100, 74)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_padded_idxs = pad_sequences(train_stories['Story_Idxs'], \n",
    "                                  maxlen=max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(train_padded_idxs) #same example story as above\n",
    "print(\"SHAPE:\", train_padded_idxs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, but we're going to create a pad_idx_seqs function that we can use later.\n",
    "\n",
    "**Exercise:** Create a pad_idx_seqs function that will return a sequence. This function will have two parameters, the sequence index and the other one which will be the length of the longest string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create a padded matrix of stories'''\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    ### ENTER YOUR CODE HERE  ( +- 2 lines)\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "    ### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the input and output\n",
    "\n",
    "In an RNN language model, the data is set up so that each word in the text is mapped to the word that follows it. In a given story, for each input word x[idx], the output label y[idx] is just x[idx+1]. In other words, the output sequences (y) matrix will be offset by one. The example below displays this alignment with the string tokens for the first story in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Output Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dan</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parents</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>were</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>overweight</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dan</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was</td>\n",
       "      <td>overweight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>overweight</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>well</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>doctors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doctors</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>told</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>his</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>parents</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>it</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>was</td>\n",
       "      <td>unhealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>unhealthy</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>his</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>parents</td>\n",
       "      <td>understood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>understood</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>and</td>\n",
       "      <td>decided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>decided</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>to</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>make</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>change</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>.</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>they</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>got</td>\n",
       "      <td>themselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>themselves</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>and</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dan</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>on</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>a</td>\n",
       "      <td>diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>diet</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Input Word Output Word\n",
       "0            -         dan\n",
       "1          dan          's\n",
       "2           's     parents\n",
       "3      parents        were\n",
       "4         were  overweight\n",
       "5   overweight           .\n",
       "6            .         dan\n",
       "7          dan         was\n",
       "8          was  overweight\n",
       "9   overweight          as\n",
       "10          as        well\n",
       "11        well           .\n",
       "12           .         the\n",
       "13         the     doctors\n",
       "14     doctors        told\n",
       "15        told         his\n",
       "16         his     parents\n",
       "17     parents          it\n",
       "18          it         was\n",
       "19         was   unhealthy\n",
       "20   unhealthy           .\n",
       "21           .         his\n",
       "22         his     parents\n",
       "23     parents  understood\n",
       "24  understood         and\n",
       "25         and     decided\n",
       "26     decided          to\n",
       "27          to        make\n",
       "28        make           a\n",
       "29           a      change\n",
       "30      change           .\n",
       "31           .        they\n",
       "32        they         got\n",
       "33         got  themselves\n",
       "34  themselves         and\n",
       "35         and         dan\n",
       "36         dan          on\n",
       "37          on           a\n",
       "38           a        diet\n",
       "39        diet           ."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip([\"-\"] + train_stories['Tokenized_Story'].loc[0],\n",
    "                          train_stories['Tokenized_Story'].loc[0])),\n",
    "                 columns=['Input Word', 'Output Word'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the padded matrices the same length, the input word matrix will also both be offset by one in the opposite direction. So the length of both the input and output matrices will be both reduced by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Input Words  Output Words\n",
      "0             0             0\n",
      "1             0             0\n",
      "2             0             0\n",
      "3             0             0\n",
      "4             0             0\n",
      "5             0             0\n",
      "6             0             0\n",
      "7             0             0\n",
      "8             0             0\n",
      "9             0             0\n",
      "10            0             0\n",
      "11            0             0\n",
      "12            0             0\n",
      "13            0             0\n",
      "14            0             0\n",
      "15            0             0\n",
      "16            0             0\n",
      "17            0             0\n",
      "18            0             0\n",
      "19            0             0\n",
      "20            0             0\n",
      "21            0             0\n",
      "22            0             0\n",
      "23            0             0\n",
      "24            0             0\n",
      "25            0             0\n",
      "26            0             0\n",
      "27            0             0\n",
      "28            0             0\n",
      "29            0             0\n",
      "..          ...           ...\n",
      "43            9            10\n",
      "44           10             7\n",
      "45            7            11\n",
      "46           11            12\n",
      "47           12            13\n",
      "48           13            14\n",
      "49           14             4\n",
      "50            4            15\n",
      "51           15             8\n",
      "52            8            16\n",
      "53           16             7\n",
      "54            7            14\n",
      "55           14             4\n",
      "56            4            17\n",
      "57           17            18\n",
      "58           18            19\n",
      "59           19            20\n",
      "60           20            21\n",
      "61           21            22\n",
      "62           22            23\n",
      "63           23             7\n",
      "64            7            24\n",
      "65           24            25\n",
      "66           25            26\n",
      "67           26            18\n",
      "68           18             2\n",
      "69            2            27\n",
      "70           27            22\n",
      "71           22            28\n",
      "72           28             7\n",
      "\n",
      "[73 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(list(zip(train_padded_idxs[0,:-1], train_padded_idxs[0, 1:])),\n",
    "                columns=['Input Words', 'Output Words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building the model\n",
    "\n",
    "To assemble the model, we'll use Keras' [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to use Keras to assemble models (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). A model consists of a series of layers. As shown in the code below, we initialize instances for each layer. Each layer can be called with another layer as input, e.g. Embedding()(input_layer). A model instance is initialized with the Model() object, which defines the initial input and final output layers for that model. Before the model can be trained, the compile() function must be called with the loss function and optimization algorithm specified (see below).\n",
    "\n",
    "### Model \n",
    "\n",
    "Until now, we have used the Squential method to create our sequential model.\n",
    "\n",
    "````\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2))\n",
    "model.add(Dense(1), activation =\"softmax\")\n",
    "````\n",
    "\n",
    "But there is another way to create a sequential model, with the Model() method\n",
    "````\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "\n",
    "input_layer = Input(shape=(2,))  \n",
    "hidden_layer = Dense(2)(input_layer)  \n",
    "output_layer = Dense(1, activation=\"softmax\")(hidden_layer )\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "````\n",
    "We will now use the second method because it is more flexible.\n",
    "\n",
    "\n",
    "### Layers\n",
    "\n",
    "We'll build an RNN with five layers:\n",
    "\n",
    "**1. Input**: The input layer takes in the matrix of word indices.\n",
    "\n",
    "**2. Embedding**: An [embedding input layer](https://keras.io/layers/embeddings/) that converts integer word indices into distributed vector representations (embeddings). The mask_zero=True parameter indicates that values of 0 in the matrix (the padding) will be ignored by the model. Check [this link for more informations](https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12)\n",
    "\n",
    "**3. GRU**: A [recurrent (GRU) hidden layer](https://keras.io/layers/recurrent/), the central component of the model. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the story at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer (see the materials at the bottom for an explanation of the difference). By setting return_sequences=True for this layer, it will output the hidden states for every timepoint in the model, i.e. for every word in the story.\n",
    "\n",
    "**4. GRU**: A second recurrent layer that takes the first as input and operates the same way, since adding more layers generally improves the model.\n",
    "\n",
    "**5. (Time Distributed) Dense**: A [dense output layer](https://keras.io/layers/core/#dense) that outputs a probability for each word in the lexicon, where each probability indicates the chance of that word being the next word in the sequence. The 'softmax' activation is what transforms the values of this layer into scores from 0 to 1 that can be treated as probabilities. The Dense layer produces the probability scores for one particular timepoint (word). By wrapping this in a TimeDistributed() layer, the model outputs a probability distribution for every timepoint in the sequence.\n",
    "\n",
    "The term \"layer\" is just an abstraction, when really all these layers are just matrices. The \"weights\" that connect the layers are also matrices. The process of training a neural network is a series of matrix multiplications. The weight matrices are the values that are adjusted during training in order for the model to learn to predict the next word.\n",
    "\n",
    "###  Parameters\n",
    "\n",
    "Our function for creating the model takes the following parameters:\n",
    "\n",
    "**seq_input_len:** the length of the input and output matrices. This is equal to the length of the longest story in the training data. \n",
    "\n",
    "**n_input_nodes**: the number of unique words in the lexicon, plus one to account for the padding represented by 0 values. This indicates the number of rows in the embedding layer, where each row corresponds to a word. It is also the dimensionality of the probability vectors given as the model output.\n",
    "\n",
    "**n_embedding_nodes**: the number of dimensions (units) in the embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layers. Like the embedding layer, this can be freely chosen. Here, it is set to 500.\n",
    "\n",
    "**stateful**: By default, the GRU hidden layer will reset its state (i.e. its values will be 0s) each time a new set of sequences is read into the model.  However, when stateful=True is given, this parameter indicates that the GRU hidden layer should \"remember\" its state until it is explicitly told to forget it. In other words, the values in this layer will be carried over between separate calls to the training function. This is useful when processing long sequences, so that the model can iterate through chunks of the sequences rather than loading the entire matrix at the same time, which is memory-intensive. I'll show below how this setting is also useful when the model is used for word prediction after training. During training, the model will observe all words in a story at once, so stateful will be set to False. At prediction time, it will be set to True.\n",
    "\n",
    "**batch_size**: It is not always necessary to specify the batch size when setting up a Keras model. The fit() function will apply batch processing by default and the batch size can be given as a parameter. However, when a model is stateful, the batch size does need to be specified in the Input() layers. Here, for training, batch_size=None, so Keras will use its default batch size (which is 32). During prediction, the batch size will be set to 1.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "The output of the model is a sequence of vectors, each with the same number of dimensions as the number of unique words (n_input_nodes). Each vector contains the predicted probability of each possible word appearing in that position in the sequence. Like all neural networks, RNNs learn by updating the parameters (weights) to optimize an objective (loss) function applied to the output. For this model, the objective is to minimize the cross-entropy (named as \"sparse_categorical_crossentropy\" in the code) between the predicted word probabilities and the probabilities observed from the words that appear in the training data, resulting in probabilities that more accurately predict when a particular word will appear. This is the general procedure used for all multi-label classification tasks. Updates to the weights of the model are performed using an optimization algorithm, such as Adam used here. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create the 4th layer which must also be a GRU. Create a variable gru_layer2 that will contain exactly the same parameters as for layer 3. Here the layer will have layer 3 as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_model(seq_input_len, n_input_nodes, n_embedding_nodes, \n",
    "                 n_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    # Layer 1\n",
    "    input_layer = Input(batch_shape=(batch_size, seq_input_len), name='input_layer')\n",
    "\n",
    "    # Layer 2\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes, \n",
    "                                output_dim=n_embedding_nodes, \n",
    "                                mask_zero=True, name='embedding_layer')(input_layer) #mask_zero=True will ignore padding\n",
    "    # Output shape = (batch_size, seq_input_len, n_embedding_nodes)\n",
    "\n",
    "    #Layer 3\n",
    "    gru_layer1 = GRU(n_hidden_nodes,\n",
    "                     return_sequences=True, #return hidden state for each word, not just last one\n",
    "                     stateful=stateful, name='hidden_layer1')(embedding_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    #Layer 4\n",
    "    gru_layer2 = GRU(n_hidden_nodes,\n",
    "                     return_sequences=True,\n",
    "                     stateful=stateful, name='hidden_layer2')(gru_layer1)\n",
    "    # Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    #Layer 5\n",
    "    output_layer = TimeDistributed(Dense(n_input_nodes, activation=\"softmax\"), \n",
    "                                   name='output_layer')(gru_layer2)\n",
    "    # Output shape = (batch_size, seq_input_len, n_input_nodes)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 14:56:30.385710 139842209699648 deprecation_wrapper.py:119] From /home/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0903 14:56:30.389559 139842209699648 deprecation_wrapper.py:119] From /home/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0903 14:56:31.269622 139842209699648 deprecation_wrapper.py:119] From /home/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0903 14:56:31.300877 139842209699648 deprecation_wrapper.py:119] From /home/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_model(seq_input_len=train_padded_idxs.shape[-1] - 1, #substract 1 from matrix length because of offset \n",
    "                     n_input_nodes = len(lexicon) + 1, # Add 1 to account for 0 padding\n",
    "                     n_embedding_nodes = 300,\n",
    "                     n_hidden_nodes = 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training\n",
    "\n",
    "Now we're ready to train the model. We'll call the fit() function to train the model for 10 iterations through the dataset (epochs), with a batch size of 20 stories. Keras reports the cross-entropy loss after each epoch - if the model is learning correctly, it should progressively decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 14:56:56.013772 139842209699648 deprecation_wrapper.py:119] From /home/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 7.1320\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 6.6825\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 6.0092\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 5.9717\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 5.8301\n"
     ]
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=train_padded_idxs[:,:-1], y=train_padded_idxs[:, 1:, None], epochs=5, batch_size=20)\n",
    "model.save_weights('example_model/model_weights.h5') #Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "It is useful to know how well the cross entropy metric applied to the training set generalizes to stories not observed during training. The typical intrinsic evaluation metric for language modeling is perplexity, which is derived from cross entropy. Perplexity is a measure of how much the model \"expects\" or \"anticipates\" the words in a given set of texts, where lower perplexity values mean the model is better at guessing the words that appear. The closer perplexity is to 0, the better the model is at predicting the given sequences. This number does not indicate the performance of a model when applied to a specific task, but it is still useful for showing relative differences between models in terms of how well they fit the data. We'll evaluate perplexity on an example test set of 100 sentences that were not observed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load test set and apply same processing used for training stories'''\n",
    "\n",
    "test_stories = pd.read_csv('dataset/example_test_stories.csv', encoding='utf-8')\n",
    "\n",
    "test_stories['Tokenized_Story'] = [[word.lower_ for word in encoder(text_seq)] for text_seq in test_stories['Story']]\n",
    "test_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=test_stories['Tokenized_Story'],\n",
    "                                            lexicon=lexicon)\n",
    "test_padded_idxs = pad_sequences(test_stories['Story_Idxs'], \n",
    "                                 maxlen=max_seq_len + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has an evaluate() function that will return the cross-entropy loss of the model on a given set of instances, the same thing that was reported during training. We can provide the test instances as input to this function to get the cross-entropy. Perplexity is equal to the exponentiated cross-entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 27ms/step\n",
      "PERPLEXITY ON TEST SET: 602.069\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "\n",
    "perplexity = numpy.exp(model.evaluate(x=test_padded_idxs[:,:-1], \n",
    "                                      y=test_padded_idxs[:, 1:, None]))\n",
    "\n",
    "print(\"PERPLEXITY ON TEST SET: {:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Tasks\n",
    "\n",
    "Now that the model is trained, we can apply it to prediction tasks using the test set. I'll show two tasks: computing a probability score for a story, and generating a new ending for a story. To demonstrate both of these, I'll load a saved model previously trained on the ~100,000 stories in the training set. As opposed to training where we processed multiple stories at the same time, it will be more straightforward to demonstrate prediction on a single story at a time, especially since prediction is fast relative to training. In Keras, you can duplicate a model by loading the parameters from a saved model into a new model. Here, this new model will have a batch size of 1. It will also process a story one word at a time (seq_input_len=1), using the stateful=True parameter to remember the story that has occurred up to that word. The other parameters of this prediction model are exactly the same as the trained model, which is why the weights can be readily transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'fawn', 13: 'lachelle', 5: 'woods', 6: 'spiders', 7: 'hanging', 8: 'francesca', 9: 'disobeying', 10: 'canes', 16670: 'aileen', 12: 'scold', 3: 'sonja', 14: 'screaming', 104: 'wooded', 16: 'grueling', 17: 'wooden', 18: 'wednesday', 19: 'crotch', 20: 'stereotypical', 20919: 'coordinate', 21: 'pizzle'}\n"
     ]
    }
   ],
   "source": [
    "'''Create a new test model, setting batch_size = 1, seq_input_len = 1, and stateful = True'''\n",
    "\n",
    "# Load lexicon from the saved model \n",
    "with open('pretrained_model/lexicon.pkl', 'rb') as f:\n",
    "    lexicon = pickle.load(f)\n",
    "lexicon_lookup = get_lexicon_lookup(lexicon)\n",
    "\n",
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_input_nodes=len(lexicon) + 1,\n",
    "                               n_embedding_nodes = 300,\n",
    "                               n_hidden_nodes = 500,\n",
    "                               stateful=True, \n",
    "                               batch_size = 1)\n",
    "\n",
    "predictor_model.load_weights('pretrained_model/model_weights.h5') #Load weights from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Re-encode the test stories with lexicon we just loaded'''\n",
    "\n",
    "test_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=test_stories['Tokenized_Story'],\n",
    "                                            lexicon=lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing story probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model outputs a probability distribution for each word in the story, indicating the probability of each possible next word in the story, we can use these values to get a single probability score for the story. To do this, we iterate through each word in a story, call the predict() function to get the full list of probabilites for the next word, and then extract the probability predicted for the actual next word in the story. We can average these probabilities across all words in the story to get a single value. The stateful=True parameter is what enables the model to remember the previous words in the story when predicting the probability of the next word. Because of this, the reset_states() function must be called at the end of reading the story in order to clear its memory for the next story.\n",
    "\n",
    "We do this below to compare the probability of each story to one with an ending randomly selected from another story in the test set. Of course, a good language model should overall score the randomly selected endings as less probable than the correct endings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STORY: lars went out skateboarding , today . he skateboarded to the skate park . his friends taught him how to do a new trick . it is a difficult trick , but he 's going to keep practicing .\n",
      "GIVEN ENDING: tomorrow , he 'll teach his friends something new , too . (P = 0.226)\n",
      "RANDOM ENDING: she bought it immediately . (P = 0.214) \n",
      "\n",
      "INITIAL STORY: abby is an avid scuba diver . abby 's dream has always been to scuba dive at the great barrier reef . one day , abby got a letter from her father in the mail . as abby opened the letter , she began crying with joy .\n",
      "GIVEN ENDING: abby 's dad was sending her to the great barrier reef to dive in may. (P = 0.255)\n",
      "RANDOM ENDING: liz still believes she almost never gets sick . (P = 0.230) \n",
      "\n",
      "INITIAL STORY: maggie was 100 years old . she knew her time was coming to an end soon . she gathered all of her family around her bed side . she told them her final goodbyes .\n",
      "GIVEN ENDING: maggie passed away minutes later . (P = 0.281)\n",
      "RANDOM ENDING: rick was the push up champion . (P = 0.251) \n",
      "\n",
      "INITIAL STORY: july is the season for peaches . we got two buckets of peaches this year . we made jam from the peaches . we also made peach sauce for ice cream !\n",
      "GIVEN ENDING: it was hard work , but all the peaches are gone now . (P = 0.151)\n",
      "RANDOM ENDING: he is glad he drank some coffee . (P = 0.145) \n",
      "\n",
      "INITIAL STORY: kim was heading to the olympics . she was a well trained athlete . a month before the games she got hurt in practice . she tried her best to get better .\n",
      "GIVEN ENDING: unfortunately she had to pull out . (P = 0.246)\n",
      "RANDOM ENDING: i guess they 'll have to find another pool to dive into . (P = 0.209) \n",
      "\n",
      "INITIAL STORY: i woke up way too early . i wanted nothing more than to go back to sleep . my back started hurting , and i could n't get comfortable . i rolled into every position imaginable .\n",
      "GIVEN ENDING: i finally gave up and got out of bed . (P = 0.285)\n",
      "RANDOM ENDING: i decided to limit my fb viewing . (P = 0.237) \n",
      "\n",
      "INITIAL STORY: my stomach was burning for several days . and i had trouble eating any kind of food . so my dad bought me a probiotic drink made with fermented cabbages . it tasted terrible .\n",
      "GIVEN ENDING: but it quickly helped my stomach . (P = 0.181)\n",
      "RANDOM ENDING: he was written up and warned . (P = 0.173) \n",
      "\n",
      "INITIAL STORY: anne was wearing her brand new dress . it cost her a fortune but she thought it was worth it . as she walked down the street , a man with a coffee cup tripped by her . spilled coffee flew toward her .\n",
      "GIVEN ENDING: anne dodged the beverage just in time , sparing her dress . (P = 0.214)\n",
      "RANDOM ENDING: tomorrow , she 'd have the song ready . (P = 0.198) \n",
      "\n",
      "INITIAL STORY: karen had two cats . her roommate had none . often the apartment smelled like cat urine . karen and her roommate got into constant argument .\n",
      "GIVEN ENDING: eventually karen moved out . (P = 0.134)\n",
      "RANDOM ENDING: she had to get the car towed and replaced . (P = 0.145) \n",
      "\n",
      "INITIAL STORY: it started out as a dark and gloomy , rainy day . i had to go to the store to grab some food for dinner . on the way i was passed by several ambulances . as i got closer to the store , i saw the ambulances were stopped .\n",
      "GIVEN ENDING: it was a horrible crash scene , and two people were dead . (P = 0.211)\n",
      "RANDOM ENDING: he had burned the pizza he forgot about . (P = 0.197) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Compute the probability of a stories according to the language model'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "def get_probability(idx_seq):\n",
    "    idx_seq = [0] + idx_seq #Prepend 0 so first call to predict() computes prob of first word from zero padding\n",
    "    probs = []\n",
    "    for word, next_word in zip(idx_seq[:-1], idx_seq[1:]):\n",
    "       # Word is an integer, but the model expects an input array\n",
    "       # with the shape (batch_size, seq_input_len), so prepend two dimensions\n",
    "        p_next_word = predictor_model.predict(numpy.array(word)[None,None])[0,0] #Output shape= (lexicon_size + 1,)\n",
    "        #Select predicted prob of the next word, which appears in the corresponding idx position of the probability vector\n",
    "        p_next_word = p_next_word[next_word]\n",
    "        probs.append(p_next_word)\n",
    "    predictor_model.reset_states()\n",
    "    return numpy.mean(probs) #return average probability of words in sequence\n",
    "\n",
    "for _, test_story in test_stories[:10].iterrows():\n",
    "\n",
    "    # Split out initial four sentences in story and ending sentence\n",
    "    len_initial_story = len([word for sent in list(encoder(test_story['Story']).sents)[:-1] for word in sent])\n",
    "    token_initial_story = test_story['Tokenized_Story'][:len_initial_story]\n",
    "    idx_initial_story = test_story['Story_Idxs'][:len_initial_story]\n",
    "    token_ending = test_story['Tokenized_Story'][len_initial_story:]\n",
    "    \n",
    "    # Randomly select another story and get its ending\n",
    "    rand_story = test_stories.loc[numpy.random.choice(len(test_stories))]\n",
    "    len_rand_ending = len(list(encoder(rand_story['Story']).sents)[-1])\n",
    "    token_rand_ending = rand_story['Tokenized_Story'][-len_rand_ending:]\n",
    "    idx_rand_ending = rand_story['Story_Idxs'][-len_rand_ending:]\n",
    "\n",
    "    print(\"INITIAL STORY:\", \" \".join(token_initial_story))\n",
    "    prob_given_ending = get_probability(test_story['Story_Idxs'])\n",
    "    print(\"GIVEN ENDING: {} (P = {:.3f})\".format(\" \".join(token_ending), prob_given_ending))\n",
    "\n",
    "    #print(\"PROBABILITY:\", get_probability(test_story['Story_Idxs']))\n",
    "    prob_rand_ending = get_probability(idx_initial_story + idx_rand_ending)\n",
    "    print(\"RANDOM ENDING: {} (P = {:.3f})\".format(\" \".join(token_rand_ending), prob_rand_ending), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sentences\n",
    "\n",
    "The language model can also be used to generate new text. Here, I'll give the same predictor model the first four sentences of a story in the test set and have it generate the fifth sentence. To do this, we \"load\" the first four sentences into the model. This can be done using predict() function. Because the model is stateful, predict() saves the representation of the story internally even though we don't need the output of this function when just reading the story. Once the final word in the fourth sentence has been read, then we can start using the resulting probability distribution to predict the first word in the fifth sentence. We can call numpy.random.choice() to randomly sample a word according to its probability. Now we again call predict() with this new word as input, which returns a probability distribution for the second word. Again, we sample from this distribution, append the newly sampled word to the previously generated word, and call predict() with this new word as input. We continue doing this until a word that ends with an end-of-sentence puncutation mark (\".\", \"!\", \"?\") has been selected. Just as before, reset_states() is called after the whole sentence has been generated. Then we can decode the generated ending into a string using the lexicon lookup dictionary. You can see that the generated endings are generally not as coherent and well-formed as the human-authored endings, but they do capture some components of the story and they are often entertaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use the model to generate new endings for stories'''\n",
    "\n",
    "def generate_ending(idx_seq):\n",
    "    \n",
    "    end_of_sent_tokens = [\".\", \"!\", \"?\"]\n",
    "    generated_ending = []\n",
    "    \n",
    "    # First just read initial story, no output needed\n",
    "    idx_seq = [0] + idx_seq #Prepend 0 so model observes 0 padding\n",
    "    for word in idx_seq:\n",
    "        p_next_word = predictor_model.predict(numpy.array(word)[None,None])[0,0]\n",
    "        \n",
    "    # Now start predicting new words\n",
    "    while not generated_ending or lexicon_lookup[next_word] not in end_of_sent_tokens:\n",
    "        #Randomly sample a word from the current probability distribution\n",
    "        next_word = numpy.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        # Append sampled word to generated ending\n",
    "        generated_ending.append(next_word)\n",
    "        # Get probabilities for next word by inputing sampled word\n",
    "        p_next_word = predictor_model.predict(numpy.array(next_word)[None,None])[0,0]\n",
    "    \n",
    "    predictor_model.reset_states() #reset hidden state after generating ending\n",
    "    \n",
    "    return generated_ending\n",
    "\n",
    "for _, test_story in test_stories[:20].iterrows():\n",
    "    # Use spacy to segment the story into sentences, so we can seperate the ending sentence\n",
    "    # Find out where in the story the ending starts (number of words from end of story)\n",
    "    ending_story_idx = len(list(encoder(test_story['Story']).sents)[-1])\n",
    "    print(\"INITIAL STORY:\", \" \".join(test_story['Tokenized_Story'][:-ending_story_idx]))\n",
    "    print(\"GIVEN ENDING:\", \" \".join(test_story['Tokenized_Story'][-ending_story_idx:]))\n",
    "    \n",
    "    generated_ending = generate_ending(test_story['Story_Idxs'][:-ending_story_idx])\n",
    "    generated_ending = \" \".join([lexicon_lookup[word] if word in lexicon_lookup else \"\"\n",
    "                                 for word in generated_ending]) #decode from numbers back into words\n",
    "    print(\"GENERATED ENDING:\", generated_ending, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data inside the model\n",
    "\n",
    "To help visualize the data representation inside the model, we can look at the output of each layer individually. Keras' Functional API lets you derive a new model with the layers from an existing model, so you can define the output to be a layer below the output layer in the original model. Calling predict() on this new model will produce the output of that layer for a given input. Of course, glancing at the numbers by themselves doesn't provide any interpretation of what the model has learned (although there are opportunities to [interpret these values](https://www.civisanalytics.com/blog/interpreting-visualizing-neural-networks-text-processing/)), but seeing them verifies the model is just a series of transformations from one matrix to another. The model stores its layers as the list model.layers, and you can retrieve specific layer by its position index in the model. Below is an example of the word embedding output for the first word in the first story of the test set. You can do this same thing to view any layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show the output of the word embedding layer for the first word of the first story'''\n",
    "\n",
    "embedding_layer = Model(inputs=predictor_model.layers[0].input,\n",
    "                        outputs=predictor_model.layers[1].output)\n",
    "embedding_output = embedding_layer.predict(numpy.array(test_stories['Story_Idxs'][0][0])[None,None])\n",
    "print(\"EMBEDDING OUTPUT SHAPE:\", embedding_output.shape)\n",
    "print(embedding_output[0]) # Print embedding vectors for first word of first story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also easy to look at the weight matrices that connect the layers. The get_weights() function will show the incoming weights for a particular layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show weights that connect the hidden layer to the output layer'''\n",
    "\n",
    "hidden_to_output_weights = predictor_model.layers[-1].get_weights()[0]\n",
    "print(\"HIDDEN-TO_OUTPUT WEIGHTS SHAPE:\", hidden_to_output_weights.shape)\n",
    "print(hidden_to_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There are a good number of tutorials on RNN language models, particularly applied to text genertion. This notebook shows how to leverage Keras with batch training when the length of the sequences is variable. There are many ways this language model can be made to be more sophisticated. Here's a few interesting papers from the NLP community that innovate this basic model for different generation tasks:\n",
    "\n",
    "*Recipe generation:* [Globally Coherent Text Generation with Neural Checklist Models](https://homes.cs.washington.edu/~yejin/Papers/emnlp16_neuralchecklist.pdf). ChloÃ© Kiddon, Luke Zettlemoyer, and Yejin Choi. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Emotional text generation:* [Affect-LM: A Neural Language Model for Customizable Affective Text Generation](https://arxiv.org/pdf/1704.06851.pdf). Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer. Annual Meeting of the Association for Computational Linguistics (ACL), 2017.\n",
    "\n",
    "*Poetry generation:* [Generating Topical Poetry](https://www.isi.edu/natural-language/mt/emnlp16-poetry.pdf). Marjan Ghazvininejad, Xing Shi, Yejin Choi, and Kevin Knight. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Dialogue generation:* [A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](http://www-etud.iro.umontreal.ca/~sordonia/pdf/naacl15.pdf). Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie1, Jianfeng Gao, Bill Dolan. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "Yoav Goldberg's book [Neural Network Methods for Natural Language Processing](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037) is a thorough introduction to neural networks for NLP tasks in general\n",
    "\n",
    "If you'd like to learn more about what Keras is doing under the hood, the [Theano tutorials](http://deeplearning.net/tutorial/) are useful. There are two specifically on RNNs for NLP: [semantic parsing](http://deeplearning.net/tutorial/rnnslu.html#rnnslu) and [sentiment analysis](http://deeplearning.net/tutorial/lstm.html#lstm)\n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is very helpful for understanding the underlying details of the same language model I've demonstrated here. It also provides raw Python code with an implementation of the backpropagation algorithm.\n",
    "\n",
    "Chris Olah provides a good [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTM RNNs work (this explanation also applies to the GRU model used here)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) documents well both the technical details of RNNs and their implementation in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
